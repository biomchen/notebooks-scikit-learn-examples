{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "========================================<br>\n", "Release Highlights for scikit-learn 0.22<br>\n", "========================================<br>\n", ".. currentmodule:: sklearn<br>\n", "We are pleased to announce the release of scikit-learn 0.22, which comes<br>\n", "with many bug fixes and new features! We detail below a few of the major<br>\n", "features of this release. For an exhaustive list of all the changes, please<br>\n", "refer to the :ref:`release notes <changes_0_22>`.<br>\n", "To install the latest version (with pip)::<br>\n", "    pip install --upgrade scikit-learn<br>\n", "or with conda::<br>\n", "    conda install scikit-learn<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "New plotting API<br>\n", "----------------<br>\n", "<br>\n", "A new plotting API is available for creating visualizations. This new API<br>\n", "allows for quickly adjusting the visuals of a plot without involving any<br>\n", "recomputation. It is also possible to add different plots to the same<br>\n", "figure. The following example illustrates :class:`~metrics.plot_roc_curve`,<br>\n", "but other plots utilities are supported like<br>\n", ":class:`~inspection.plot_partial_dependence`,<br>\n", ":class:`~metrics.plot_precision_recall_curve`, and<br>\n", ":class:`~metrics.plot_confusion_matrix`. Read more about this new API in the<br>\n", ":ref:`User Guide <visualizations>`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.svm import SVC\n", "from sklearn.metrics import plot_roc_curve\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.datasets import make_classification\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = make_classification(random_state=0)\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["svc = SVC(random_state=42)\n", "svc.fit(X_train, y_train)\n", "rfc = RandomForestClassifier(random_state=42)\n", "rfc.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["svc_disp = plot_roc_curve(svc, X_test, y_test)\n", "rfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=svc_disp.ax_)\n", "rfc_disp.figure_.suptitle(\"ROC curve comparison\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##########################################################################<br>\n", "Stacking Classifier and Regressor<br>\n", "---------------------------------<br>\n", ":class:`~ensemble.StackingClassifier` and<br>\n", ":class:`~ensemble.StackingRegressor`<br>\n", "allow you to have a stack of estimators with a final classifier or<br>\n", "a regressor.<br>\n", "Stacked generalization consists in stacking the output of individual<br>\n", "estimators and use a classifier to compute the final prediction. Stacking<br>\n", "allows to use the strength of each individual estimator by using their output<br>\n", "as input of a final estimator.<br>\n", "Base estimators are fitted on the full ``X`` while<br>\n", "the final estimator is trained using cross-validated predictions of the<br>\n", "base estimators using ``cross_val_predict``.<br>\n", "<br>\n", "Read more in the :ref:`User Guide <stacking>`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_iris\n", "from sklearn.svm import LinearSVC\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.pipeline import make_pipeline\n", "from sklearn.ensemble import StackingClassifier\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = load_iris(return_X_y=True)\n", "estimators = [\n", "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n", "    ('svr', make_pipeline(StandardScaler(),\n", "                          LinearSVC(random_state=42)))\n", "]\n", "clf = StackingClassifier(\n", "    estimators=estimators, final_estimator=LogisticRegression()\n", ")\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, stratify=y, random_state=42\n", ")\n", "clf.fit(X_train, y_train).score(X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Permutation-based feature importance<br>\n", "------------------------------------<br>\n", "<br>\n", "The :func:`inspection.permutation_importance` can be used to get an<br>\n", "estimate of the importance of each feature, for any fitted estimator:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.inspection import permutation_importance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = make_classification(random_state=0, n_features=5, n_informative=3)\n", "rf = RandomForestClassifier(random_state=0).fit(X, y)\n", "result = permutation_importance(rf, X, y, n_repeats=10, random_state=0,\n", "                                n_jobs=-1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "sorted_idx = result.importances_mean.argsort()\n", "ax.boxplot(result.importances[sorted_idx].T,\n", "           vert=False, labels=range(X.shape[1]))\n", "ax.set_title(\"Permutation Importance of each feature\")\n", "ax.set_ylabel(\"Features\")\n", "fig.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Native support for missing values for gradient boosting<br>\n", "-------------------------------------------------------<br>\n", "<br>\n", "The :class:`ensemble.HistGradientBoostingClassifier`<br>\n", "and :class:`ensemble.HistGradientBoostingRegressor` now have native<br>\n", "support for missing values (NaNs). This means that there is no need for<br>\n", "imputing data when training or predicting."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n", "from sklearn.ensemble import HistGradientBoostingClassifier\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\n", "y = [0, 0, 1, 1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\n", "print(gbdt.predict(X))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##########################################################################<br>\n", "Precomputed sparse nearest neighbors graph<br>\n", "------------------------------------------<br>\n", "Most estimators based on nearest neighbors graphs now accept precomputed<br>\n", "sparse graphs as input, to reuse the same graph for multiple estimator fits.<br>\n", "To use this feature in a pipeline, one can use the `memory` parameter, along<br>\n", "with one of the two new transformers,<br>\n", ":class:`neighbors.KNeighborsTransformer` and<br>\n", ":class:`neighbors.RadiusNeighborsTransformer`. The precomputation<br>\n", "can also be performed by custom estimators to use alternative<br>\n", "implementations, such as approximate nearest neighbors methods.<br>\n", "See more details in the :ref:`User Guide <neighbors_transformer>`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tempfile import TemporaryDirectory\n", "from sklearn.neighbors import KNeighborsTransformer\n", "from sklearn.manifold import Isomap\n", "from sklearn.pipeline import make_pipeline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = make_classification(random_state=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with TemporaryDirectory(prefix=\"sklearn_cache_\") as tmpdir:\n", "    estimator = make_pipeline(\n", "        KNeighborsTransformer(n_neighbors=10, mode='distance'),\n", "        Isomap(n_neighbors=10, metric='precomputed'),\n", "        memory=tmpdir)\n", "    estimator.fit(X)\n\n", "    # We can decrease the number of neighbors and the graph will not be\n", "    # recomputed.\n", "    estimator.set_params(isomap__n_neighbors=5)\n", "    estimator.fit(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "KNN Based Imputation<br>\n", "------------------------------------<br>\n", "We now support imputation for completing missing values using k-Nearest<br>\n", "Neighbors.<br>\n", "<br>\n", "Each sample's missing values are imputed using the mean value from<br>\n", "``n_neighbors`` nearest neighbors found in the training set. Two samples are<br>\n", "close if the features that neither is missing are close.<br>\n", "By default, a euclidean distance metric<br>\n", "that supports missing values,<br>\n", ":func:`~metrics.nan_euclidean_distances`, is used to find the nearest<br>\n", "neighbors.<br>\n", "<br>\n", "Read more in the :ref:`User Guide <knnimpute>`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.impute import KNNImputer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n", "imputer = KNNImputer(n_neighbors=2)\n", "print(imputer.fit_transform(X))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###########################################################################<br>\n", "Tree pruning<br>\n", "------------<br>\n", "<br>\n", "It is now possible to prune most tree-based estimators once the trees are<br>\n", "built. The pruning is based on minimal cost-complexity. Read more in the<br>\n", ":ref:`User Guide <minimal_cost_complexity_pruning>` for details."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = make_classification(random_state=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\n", "print(\"Average number of nodes without pruning {:.1f}\".format(\n", "    np.mean([e.tree_.node_count for e in rf.estimators_])))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rf = RandomForestClassifier(random_state=0, ccp_alpha=0.05).fit(X, y)\n", "print(\"Average number of nodes with pruning {:.1f}\".format(\n", "    np.mean([e.tree_.node_count for e in rf.estimators_])))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##########################################################################<br>\n", "Retrieve dataframes from OpenML<br>\n", "-------------------------------<br>\n", ":func:`datasets.fetch_openml` can now return pandas dataframe and thus<br>\n", "properly handle datasets with heterogeneous data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import fetch_openml"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["titanic = fetch_openml('titanic', version=1, as_frame=True)\n", "print(titanic.data.head()[['pclass', 'embarked']])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##########################################################################<br>\n", "Checking scikit-learn compatibility of an estimator<br>\n", "---------------------------------------------------<br>\n", "Developers can check the compatibility of their scikit-learn compatible<br>\n", "estimators using :func:`~utils.estimator_checks.check_estimator`. For<br>\n", "instance, the ``check_estimator(LinearSVC)`` passes.<br>\n", "<br>\n", "We now provide a ``pytest`` specific decorator which allows ``pytest``<br>\n", "to run all checks independently and report the checks that are failing."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.utils.estimator_checks import parametrize_with_checks"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@parametrize_with_checks([LogisticRegression, DecisionTreeRegressor])\n", "def test_sklearn_compatible_estimator(estimator, check):\n", "    check(estimator)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##########################################################################<br>\n", "ROC AUC now supports multiclass classification<br>\n", "----------------------------------------------<br>\n", "The :func:`roc_auc_score` function can also be used in multi-class<br>\n", "classification. Two averaging strategies are currently supported: the<br>\n", "one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and<br>\n", "the one-vs-rest algorithm computes the average of the ROC AUC scores for each<br>\n", "class against all other classes. In both cases, the multiclass ROC AUC scores<br>\n", "are computed from the probability estimates that a sample belongs to a<br>\n", "particular class according to the model. The OvO and OvR algorithms support<br>\n", "weighting uniformly (``average='macro'``) and weighting by the prevalence<br>\n", "(``average='weighted'``).<br>\n", "<br>\n", "Read more in the :ref:`User Guide <roc_metrics>`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_classification\n", "from sklearn.svm import SVC\n", "from sklearn.metrics import roc_auc_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = make_classification(n_classes=4, n_informative=16)\n", "clf = SVC(decision_function_shape='ovo', probability=True).fit(X, y)\n", "print(roc_auc_score(y, clf.predict_proba(X), multi_class='ovo'))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}