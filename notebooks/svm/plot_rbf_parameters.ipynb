{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "==================<br>\n", "RBF SVM parameters<br>\n", "==================<br>\n", "This example illustrates the effect of the parameters ``gamma`` and ``C`` of<br>\n", "the Radial Basis Function (RBF) kernel SVM.<br>\n", "Intuitively, the ``gamma`` parameter defines how far the influence of a single<br>\n", "training example reaches, with low values meaning 'far' and high values meaning<br>\n", "'close'. The ``gamma`` parameters can be seen as the inverse of the radius of<br>\n", "influence of samples selected by the model as support vectors.<br>\n", "The ``C`` parameter trades off correct classification of training examples<br>\n", "against maximization of the decision function's margin. For larger values of<br>\n", "``C``, a smaller margin will be accepted if the decision function is better at<br>\n", "classifying all training points correctly. A lower ``C`` will encourage a<br>\n", "larger margin, therefore a simpler decision function, at the cost of training<br>\n", "accuracy. In other words``C`` behaves as a regularization parameter in the<br>\n", "SVM.<br>\n", "The first plot is a visualization of the decision function for a variety of<br>\n", "parameter values on a simplified classification problem involving only 2 input<br>\n", "features and 2 possible target classes (binary classification). Note that this<br>\n", "kind of plot is not possible to do for problems with more features or target<br>\n", "classes.<br>\n", "The second plot is a heatmap of the classifier's cross-validation accuracy as a<br>\n", "function of ``C`` and ``gamma``. For this example we explore a relatively large<br>\n", "grid for illustration purposes. In practice, a logarithmic grid from<br>\n", ":math:`10^{-3}` to :math:`10^3` is usually sufficient. If the best parameters<br>\n", "lie on the boundaries of the grid, it can be extended in that direction in a<br>\n", "subsequent search.<br>\n", "Note that the heat map plot has a special colorbar with a midpoint value close<br>\n", "to the score values of the best performing models so as to make it easy to tell<br>\n", "them apart in the blink of an eye.<br>\n", "The behavior of the model is very sensitive to the ``gamma`` parameter. If<br>\n", "``gamma`` is too large, the radius of the area of influence of the support<br>\n", "vectors only includes the support vector itself and no amount of<br>\n", "regularization with ``C`` will be able to prevent overfitting.<br>\n", "When ``gamma`` is very small, the model is too constrained and cannot capture<br>\n", "the complexity or \"shape\" of the data. The region of influence of any selected<br>\n", "support vector would include the whole training set. The resulting model will<br>\n", "behave similarly to a linear model with a set of hyperplanes that separate the<br>\n", "centers of high density of any pair of two classes.<br>\n", "For intermediate values, we can see on the second plot that good models can<br>\n", "be found on a diagonal of ``C`` and ``gamma``. Smooth models (lower ``gamma``<br>\n", "values) can be made more complex by increasing the importance of classifying<br>\n", "each point correctly (larger ``C`` values) hence the diagonal of good<br>\n", "performing models.<br>\n", "Finally one can also observe that for some intermediate values of ``gamma`` we<br>\n", "get equally performing models when ``C`` becomes very large: it is not<br>\n", "necessary to regularize by enforcing a larger margin. The radius of the RBF<br>\n", "kernel alone acts as a good structural regularizer. In practice though it<br>\n", "might still be interesting to simplify the decision function with a lower<br>\n", "value of ``C`` so as to favor models that use less memory and that are faster<br>\n", "to predict.<br>\n", "We should also note that small differences in scores results from the random<br>\n", "splits of the cross-validation procedure. Those spurious variations can be<br>\n", "smoothed out by increasing the number of CV iterations ``n_splits`` at the<br>\n", "expense of compute time. Increasing the value number of ``C_range`` and<br>\n", "``gamma_range`` steps will increase the resolution of the hyper-parameter heat<br>\n", "map.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from matplotlib.colors import Normalize"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.svm import SVC\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.datasets import load_iris\n", "from sklearn.model_selection import StratifiedShuffleSplit\n", "from sklearn.model_selection import GridSearchCV"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Utility function to move the midpoint of a colormap to be around<br>\n", "the values of interest."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MidpointNormalize(Normalize):\n", "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n", "        self.midpoint = midpoint\n", "        Normalize.__init__(self, vmin, vmax, clip)\n", "    def __call__(self, value, clip=None):\n", "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n", "        return np.ma.masked_array(np.interp(value, x, y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Load and prepare data set<br>\n", "<br>\n", "dataset for grid search"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["iris = load_iris()\n", "X = iris.data\n", "y = iris.target"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dataset for decision function visualization: we only keep the first two<br>\n", "features in X and sub-sample the dataset to keep only 2 classes and<br>\n", "make it a binary classification problem."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_2d = X[:, :2]\n", "X_2d = X_2d[y > 0]\n", "y_2d = y[y > 0]\n", "y_2d -= 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is usually a good idea to scale the data for SVM training.<br>\n", "We are cheating a bit in this example in scaling all of the data,<br>\n", "instead of fitting the transformation on the training set and<br>\n", "just applying it on the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\n", "X = scaler.fit_transform(X)\n", "X_2d = scaler.fit_transform(X_2d)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Train classifiers<br>\n", "<br>\n", "For an initial search, a logarithmic grid with basis<br>\n", "10 is often helpful. Using a basis of 2, a finer<br>\n", "tuning can be achieved but at a much higher cost."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["C_range = np.logspace(-2, 10, 13)\n", "gamma_range = np.logspace(-9, 3, 13)\n", "param_grid = dict(gamma=gamma_range, C=C_range)\n", "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n", "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n", "grid.fit(X, y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"The best parameters are %s with a score of %0.2f\"\n", "      % (grid.best_params_, grid.best_score_))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we need to fit a classifier for all parameters in the 2d version<br>\n", "(we use a smaller set of parameters here because it takes a while to train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["C_2d_range = [1e-2, 1, 1e2]\n", "gamma_2d_range = [1e-1, 1, 1e1]\n", "classifiers = []\n", "for C in C_2d_range:\n", "    for gamma in gamma_2d_range:\n", "        clf = SVC(C=C, gamma=gamma)\n", "        clf.fit(X_2d, y_2d)\n", "        classifiers.append((C, gamma, clf))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Visualization<br>\n", "<br>\n", "draw visualization of parameter effects"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8, 6))\n", "xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\n", "for (k, (C, gamma, clf)) in enumerate(classifiers):\n", "    # evaluate decision function in a grid\n", "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n", "    Z = Z.reshape(xx.shape)\n\n", "    # visualize decision function for these parameters\n", "    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n", "    plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)),\n", "              size='medium')\n\n", "    # visualize parameter's effect on decision function\n", "    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n", "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r,\n", "                edgecolors='k')\n", "    plt.xticks(())\n", "    plt.yticks(())\n", "    plt.axis('tight')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),\n", "                                                     len(gamma_range))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Draw heatmap of the validation accuracy as a function of gamma and C<br>\n", "<br>\n", "The score are encoded as colors with the hot colormap which varies from dark<br>\n", "red to bright yellow. As the most interesting scores are all located in the<br>\n", "0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so<br>\n", "as to make it easier to visualize the small variations of score values in the<br>\n", "interesting range while not brutally collapsing all the low score values to<br>\n", "the same color."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8, 6))\n", "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n", "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n", "           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n", "plt.xlabel('gamma')\n", "plt.ylabel('C')\n", "plt.colorbar()\n", "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n", "plt.yticks(np.arange(len(C_range)), C_range)\n", "plt.title('Validation accuracy')\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}