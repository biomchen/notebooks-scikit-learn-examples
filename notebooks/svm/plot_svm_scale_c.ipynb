{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["r\n<br>\n", "==============================================<br>\n", "Scaling the regularization parameter for SVCs<br>\n", "==============================================<br>\n", "The following example illustrates the effect of scaling the<br>\n", "regularization parameter when using :ref:`svm` for<br>\n", ":ref:`classification <svm_classification>`.<br>\n", "For SVC classification, we are interested in a risk minimization for the<br>\n", "equation:<br>\n", ".. math::<br>\n", "    C \\sum_{i=1, n} \\mathcal{L} (f(x_i), y_i) + \\Omega (w)<br>\n", "where<br>\n", "    - :math:`C` is used to set the amount of regularization<br>\n", "    - :math:`\\mathcal{L}` is a `loss` function of our samples<br>\n", "      and our model parameters.<br>\n", "    - :math:`\\Omega` is a `penalty` function of our model parameters<br>\n", "If we consider the loss function to be the individual error per<br>\n", "sample, then the data-fit term, or the sum of the error for each sample, will<br>\n", "increase as we add more samples. The penalization term, however, will not<br>\n", "increase.<br>\n", "When using, for example, :ref:`cross validation <cross_validation>`, to<br>\n", "set the amount of regularization with `C`, there will be a<br>\n", "different amount of samples between the main problem and the smaller problems<br>\n", "within the folds of the cross validation.<br>\n", "Since our loss function is dependent on the amount of samples, the latter<br>\n", "will influence the selected value of `C`.<br>\n", "The question that arises is `How do we optimally adjust C to<br>\n", "account for the different amount of training samples?`<br>\n", "The figures below are used to illustrate the effect of scaling our<br>\n", "`C` to compensate for the change in the number of samples, in the<br>\n", "case of using an `l1` penalty, as well as the `l2` penalty.<br>\n", "l1-penalty case<br>\n", "-----------------<br>\n", "In the `l1` case, theory says that prediction consistency<br>\n", "(i.e. that under given hypothesis, the estimator<br>\n", "learned predicts as well as a model knowing the true distribution)<br>\n", "is not possible because of the bias of the `l1`. It does say, however,<br>\n", "that model consistency, in terms of finding the right set of non-zero<br>\n", "parameters as well as their signs, can be achieved by scaling<br>\n", "`C1`.<br>\n", "l2-penalty case<br>\n", "-----------------<br>\n", "The theory says that in order to achieve prediction consistency, the<br>\n", "penalty parameter should be kept constant<br>\n", "as the number of samples grow.<br>\n", "Simulations<br>\n", "------------<br>\n", "The two figures below plot the values of `C` on the `x-axis` and the<br>\n", "corresponding cross-validation scores on the `y-axis`, for several different<br>\n", "fractions of a generated data-set.<br>\n", "In the `l1` penalty case, the cross-validation-error correlates best with<br>\n", "the test-error, when scaling our `C` with the number of samples, `n`,<br>\n", "which can be seen in the first figure.<br>\n", "For the `l2` penalty case, the best result comes from the case where `C`<br>\n", "is not scaled.<br>\n", ".. topic:: Note:<br>\n", "    Two separate datasets are used for the two different plots. The reason<br>\n", "    behind this is the `l1` case works better on sparse data, while `l2`<br>\n", "    is better suited to the non-sparse case.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Andreas Mueller <amueller@ais.uni-bonn.de><br>\n", "        Jaques Grobler <jaques.grobler@inria.fr><br>\n", "License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.svm import LinearSVC\n", "from sklearn.model_selection import ShuffleSplit\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.utils import check_random_state\n", "from sklearn import datasets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rnd = check_random_state(1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["set up dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples = 100\n", "n_features = 300"]}, {"cell_type": "markdown", "metadata": {}, "source": ["l1 data (only 5 informative features)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_1, y_1 = datasets.make_classification(n_samples=n_samples,\n", "                                        n_features=n_features, n_informative=5,\n", "                                        random_state=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["l2 data: non sparse, but less features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_2 = np.sign(.5 - rnd.rand(n_samples))\n", "X_2 = rnd.randn(n_samples, n_features // 5) + y_2[:, np.newaxis]\n", "X_2 += 5 * rnd.randn(n_samples, n_features // 5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,\n", "                       tol=1e-3),\n", "             np.logspace(-2.3, -1.3, 10), X_1, y_1),\n", "            (LinearSVC(penalty='l2', loss='squared_hinge', dual=True),\n", "             np.logspace(-4.5, -2, 10), X_2, y_2)]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["colors = ['navy', 'cyan', 'darkorange']\n", "lw = 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for clf, cs, X, y in clf_sets:\n", "    # set up the plot for each regressor\n", "    fig, axes = plt.subplots(nrows=2, sharey=True, figsize=(9, 10))\n", "    for k, train_size in enumerate(np.linspace(0.3, 0.7, 3)[::-1]):\n", "        param_grid = dict(C=cs)\n", "        # To get nice curve, we need a large number of iterations to\n", "        # reduce the variance\n", "        grid = GridSearchCV(clf, refit=False, param_grid=param_grid,\n", "                            cv=ShuffleSplit(train_size=train_size,\n", "                                            test_size=.3,\n", "                                            n_splits=250, random_state=1))\n", "        grid.fit(X, y)\n", "        scores = grid.cv_results_['mean_test_score']\n", "        scales = [(1, 'No scaling'),\n", "                  ((n_samples * train_size), '1/n_samples'),\n", "                  ]\n", "        for ax, (scaler, name) in zip(axes, scales):\n", "            ax.set_xlabel('C')\n", "            ax.set_ylabel('CV Score')\n", "            grid_cs = cs * float(scaler)  # scale the C's\n", "            ax.semilogx(grid_cs, scores, label=\"fraction %.2f\" %\n", "                        train_size, color=colors[k], lw=lw)\n", "            ax.set_title('scaling=%s, penalty=%s, loss=%s' %\n", "                         (name, clf.penalty, clf.loss))\n", "    plt.legend(loc=\"best\")\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}