{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "==============================================<br>\n", "L1 Penalty and Sparsity in Logistic Regression<br>\n", "==============================================<br>\n", "Comparison of the sparsity (percentage of zero coefficients) of solutions when<br>\n", "L1, L2 and Elastic-Net penalty are used for different values of C. We can see<br>\n", "that large values of C give more freedom to the model.  Conversely, smaller<br>\n", "values of C constrain the model more. In the L1 penalty case, this leads to<br>\n", "sparser solutions. As expected, the Elastic-Net penalty sparsity is between<br>\n", "that of L1 and L2.<br>\n", "We classify 8x8 images of digits into two classes: 0-4 against 5-9.<br>\n", "The visualization shows coefficients of the models for varying C.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr><br>\n", "         Mathieu Blondel <mathieu@mblondel.org><br>\n", "         Andreas Mueller <amueller@ais.uni-bonn.de><br>\n", "License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn import datasets\n", "from sklearn.preprocessing import StandardScaler"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = datasets.load_digits(return_X_y=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = StandardScaler().fit_transform(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["classify small against large digits"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = (y > 4).astype(np.int)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["l1_ratio = 0.5  # L1 weight in the Elastic-Net regularization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(3, 3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set regularization parameter"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):\n", "    # turn down tolerance for short training time\n", "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01, solver='saga')\n", "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01, solver='saga')\n", "    clf_en_LR = LogisticRegression(C=C, penalty='elasticnet', solver='saga',\n", "                                   l1_ratio=l1_ratio, tol=0.01)\n", "    clf_l1_LR.fit(X, y)\n", "    clf_l2_LR.fit(X, y)\n", "    clf_en_LR.fit(X, y)\n", "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n", "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n", "    coef_en_LR = clf_en_LR.coef_.ravel()\n\n", "    # coef_l1_LR contains zeros due to the\n", "    # L1 sparsity inducing norm\n", "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n", "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n", "    sparsity_en_LR = np.mean(coef_en_LR == 0) * 100\n", "    print(\"C=%.2f\" % C)\n", "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L1 penalty:\", sparsity_l1_LR))\n", "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with Elastic-Net penalty:\",\n", "                                  sparsity_en_LR))\n", "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L2 penalty:\", sparsity_l2_LR))\n", "    print(\"{:<40} {:.2f}\".format(\"Score with L1 penalty:\",\n", "                                 clf_l1_LR.score(X, y)))\n", "    print(\"{:<40} {:.2f}\".format(\"Score with Elastic-Net penalty:\",\n", "                                 clf_en_LR.score(X, y)))\n", "    print(\"{:<40} {:.2f}\".format(\"Score with L2 penalty:\",\n", "                                 clf_l2_LR.score(X, y)))\n", "    if i == 0:\n", "        axes_row[0].set_title(\"L1 penalty\")\n", "        axes_row[1].set_title(\"Elastic-Net\\nl1_ratio = %s\" % l1_ratio)\n", "        axes_row[2].set_title(\"L2 penalty\")\n", "    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):\n", "        ax.imshow(np.abs(coefs.reshape(8, 8)), interpolation='nearest',\n", "                  cmap='binary', vmax=1, vmin=0)\n", "        ax.set_xticks(())\n", "        ax.set_yticks(())\n", "    axes_row[0].set_ylabel('C = %s' % C)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}