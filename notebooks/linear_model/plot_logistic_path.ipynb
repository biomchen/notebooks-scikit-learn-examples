{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "\"\"\"\n", "==============================================\n", "Regularization path of L1- Logistic Regression\n", "=============================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Train l1-penalized logistic regression models on a binary classification\n", "problem derived from the Iris dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["The models are ordered from strongest regularized to least regularized. The 4\n", "coefficients of the models are collected and plotted as a \"regularization\n", "path\": on the left-hand side of the figure (strong regularizers), all the\n", "coefficients are exactly 0. When regularization gets progressively looser,\n", "coefficients can get non-zero values one after the other."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Here we choose the liblinear solver because it can efficiently optimize for the\n", "Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Also note that we set a low value for the tolerance to make sure that the model\n", "has converged before collecting the coefficients."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["We also use warm_start=True which means that the coefficients of the models are\n", "reused to initialize the next model fit to speed-up the computation of the\n", "full-path."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "print(__doc__)<br>\n", "# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr><br>\n", "# License: BSD 3 clause<br>\n", "from time import time<br>\n", "import numpy as np<br>\n", "import matplotlib.pyplot as plt<br>\n", "from sklearn import linear_model<br>\n", "from sklearn import datasets<br>\n", "from sklearn.svm import l1_min_c<br>\n", "iris = datasets.load_iris()<br>\n", "X = iris.data<br>\n", "y = iris.target<br>\n", "X = X[y != 2]<br>\n", "y = y[y != 2]<br>\n", "X /= X.max()  # Normalize X to speed-up convergence<br>\n", "# #############################################################################<br>\n", "# Demo path functions<br>\n", "cs = l1_min_c(X, y, loss='log') * np.logspace(0, 7, 16)<br>\n", "print(\"Computing regularization path ...\")<br>\n", "start = time()<br>\n", "clf = linear_model.LogisticRegression(penalty='l1', solver='liblinear',<br>\n", "                                      tol=1e-6, max_iter=int(1e6),<br>\n", "                                      warm_start=True,<br>\n", "                                      intercept_scaling=10000.)<br>\n", "coefs_ = []<br>\n", "for c in cs:<br>\n", "    clf.set_params(C=c)<br>\n", "    clf.fit(X, y)<br>\n", "    coefs_.append(clf.coef_.ravel().copy())<br>\n", "print(\"This took %0.3fs\" % (time() - start))<br>\n", "coefs_ = np.array(coefs_)<br>\n", "plt.plot(np.log10(cs), coefs_, marker='o')<br>\n", "ymin, ymax = plt.ylim()<br>\n", "plt.xlabel('log(C)')<br>\n", "plt.ylabel('Coefficients')<br>\n", "plt.title('Logistic Regression Path')<br>\n", "plt.axis('tight')<br>\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}