{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "===========================================<br>\n", "FeatureHasher and DictVectorizer Comparison<br>\n", "===========================================<br>\n", "Compares FeatureHasher and DictVectorizer by using both to vectorize<br>\n", "text documents.<br>\n", "The example demonstrates syntax and speed only; it doesn't actually do<br>\n", "anything useful with the extracted vectors. See the example scripts<br>\n", "{document_classification_20newsgroups,clustering}.py for actual learning<br>\n", "on text documents.<br>\n", "A discrepancy between the number of terms reported for DictVectorizer and<br>\n", "for FeatureHasher is to be expected due to hash collisions.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Lars Buitinck<br>\n", "License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from collections import defaultdict\n", "import re\n", "import sys\n", "from time import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import fetch_20newsgroups\n", "from sklearn.feature_extraction import DictVectorizer, FeatureHasher"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def n_nonzero_columns(X):\n", "    \"\"\"Returns the number of non-zero columns in a CSR matrix X.\"\"\"\n", "    return len(np.unique(X.nonzero()[1]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tokens(doc):\n", "    \"\"\"Extract tokens from doc.\n", "    This uses a simple regex to break strings into tokens. For a more\n", "    principled approach, see CountVectorizer or TfidfVectorizer.\n", "    \"\"\"\n", "    return (tok.lower() for tok in re.findall(r\"\\w+\", doc))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def token_freqs(doc):\n", "    \"\"\"Extract a dict mapping tokens from doc to their frequencies.\"\"\"\n", "    freq = defaultdict(int)\n", "    for tok in tokens(doc):\n", "        freq[tok] += 1\n", "    return freq"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["categories = [\n", "    'alt.atheism',\n", "    'comp.graphics',\n", "    'comp.sys.ibm.pc.hardware',\n", "    'misc.forsale',\n", "    'rec.autos',\n", "    'sci.space',\n", "    'talk.religion.misc',\n", "]\n", "# Uncomment the following line to use a larger set (11k+ documents)\n", "# categories = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)\n", "print(\"Usage: %s [n_features_for_hashing]\" % sys.argv[0])\n", "print(\"    The default number of features is 2**18.\")\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    n_features = int(sys.argv[1])\n", "except IndexError:\n", "    n_features = 2 ** 18\n", "except ValueError:\n", "    print(\"not a valid number of features: %r\" % sys.argv[1])\n", "    sys.exit(1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Loading 20 newsgroups training data\")\n", "raw_data, _ = fetch_20newsgroups(subset='train', categories=categories,\n", "                                 return_X_y=True)\n", "data_size_mb = sum(len(s.encode('utf-8')) for s in raw_data) / 1e6\n", "print(\"%d documents - %0.3fMB\" % (len(raw_data), data_size_mb))\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"DictVectorizer\")\n", "t0 = time()\n", "vectorizer = DictVectorizer()\n", "vectorizer.fit_transform(token_freqs(d) for d in raw_data)\n", "duration = time() - t0\n", "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n", "print(\"Found %d unique terms\" % len(vectorizer.get_feature_names()))\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"FeatureHasher on frequency dicts\")\n", "t0 = time()\n", "hasher = FeatureHasher(n_features=n_features)\n", "X = hasher.transform(token_freqs(d) for d in raw_data)\n", "duration = time() - t0\n", "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n", "print(\"Found %d unique terms\" % n_nonzero_columns(X))\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"FeatureHasher on raw tokens\")\n", "t0 = time()\n", "hasher = FeatureHasher(n_features=n_features, input_type=\"string\")\n", "X = hasher.transform(tokens(d) for d in raw_data)\n", "duration = time() - t0\n", "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n", "print(\"Found %d unique terms\" % n_nonzero_columns(X))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}