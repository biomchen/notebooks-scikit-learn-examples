{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "=======================================<br>\n", "Clustering text documents using k-means<br>\n", "=======================================<br>\n", "This is an example showing how the scikit-learn can be used to cluster<br>\n", "documents by topics using a bag-of-words approach. This example uses<br>\n", "a scipy.sparse matrix to store the features instead of standard numpy arrays.<br>\n", "Two feature extraction methods can be used in this example:<br>\n", "  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most<br>\n", "    frequent words to features indices and hence compute a word occurrence<br>\n", "    frequency (sparse) matrix. The word frequencies are then reweighted using<br>\n", "    the Inverse Document Frequency (IDF) vector collected feature-wise over<br>\n", "    the corpus.<br>\n", "  - HashingVectorizer hashes word occurrences to a fixed dimensional space,<br>\n", "    possibly with collisions. The word count vectors are then normalized to<br>\n", "    each have l2-norm equal to one (projected to the euclidean unit-ball) which<br>\n", "    seems to be important for k-means to work in high dimensional space.<br>\n", "    HashingVectorizer does not provide IDF weighting as this is a stateless<br>\n", "    model (the fit method does nothing). When IDF weighting is needed it can<br>\n", "    be added by pipelining its output to a TfidfTransformer instance.<br>\n", "Two algorithms are demoed: ordinary k-means and its more scalable cousin<br>\n", "minibatch k-means.<br>\n", "Additionally, latent semantic analysis can also be used to reduce<br>\n", "dimensionality and discover latent patterns in the data.<br>\n", "It can be noted that k-means (and minibatch k-means) are very sensitive to<br>\n", "feature scaling and that in this case the IDF weighting helps improve the<br>\n", "quality of the clustering by quite a lot as measured against the \"ground truth\"<br>\n", "provided by the class label assignments of the 20 newsgroups dataset.<br>\n", "This improvement is not visible in the Silhouette Coefficient which is small<br>\n", "for both as this measure seem to suffer from the phenomenon called<br>\n", "\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional<br>\n", "datasets such as text data. Other measures such as V-measure and Adjusted Rand<br>\n", "Index are information theoretic based evaluation scores: as they are only based<br>\n", "on cluster assignments rather than distances, hence not affected by the curse<br>\n", "of dimensionality.<br>\n", "Note: as k-means is optimizing a non-convex objective function, it will likely<br>\n", "end up in a local optimum. Several runs with independent random init might be<br>\n", "necessary to get a good convergence.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Peter Prettenhofer <peter.prettenhofer@gmail.com><br>\n", "        Lars Buitinck<br>\n", "License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import fetch_20newsgroups\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.feature_extraction.text import HashingVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "from sklearn.pipeline import make_pipeline\n", "from sklearn.preprocessing import Normalizer\n", "from sklearn import metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans, MiniBatchKMeans"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging\n", "from optparse import OptionParser\n", "import sys\n", "from time import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Display progress logs on stdout"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["logging.basicConfig(level=logging.INFO,\n", "                    format='%(asctime)s %(levelname)s %(message)s')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["parse commandline arguments"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["op = OptionParser()\n", "op.add_option(\"--lsa\",\n", "              dest=\"n_components\", type=\"int\",\n", "              help=\"Preprocess documents with latent semantic analysis.\")\n", "op.add_option(\"--no-minibatch\",\n", "              action=\"store_false\", dest=\"minibatch\", default=True,\n", "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n", "op.add_option(\"--no-idf\",\n", "              action=\"store_false\", dest=\"use_idf\", default=True,\n", "              help=\"Disable Inverse Document Frequency feature weighting.\")\n", "op.add_option(\"--use-hashing\",\n", "              action=\"store_true\", default=False,\n", "              help=\"Use a hashing feature vectorizer\")\n", "op.add_option(\"--n-features\", type=int, default=10000,\n", "              help=\"Maximum number of features (dimensions)\"\n", "                   \" to extract from text.\")\n", "op.add_option(\"--verbose\",\n", "              action=\"store_true\", dest=\"verbose\", default=False,\n", "              help=\"Print progress reports inside k-means algorithm.\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)\n", "op.print_help()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def is_interactive():\n", "    return not hasattr(sys.modules['__main__'], '__file__')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["work-around for Jupyter notebook and IPython console"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["argv = [] if is_interactive() else sys.argv[1:]\n", "(opts, args) = op.parse_args(argv)\n", "if len(args) > 0:\n", "    op.error(\"this script takes no arguments.\")\n", "    sys.exit(1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Load some categories from the training set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["categories = [\n", "    'alt.atheism',\n", "    'talk.religion.misc',\n", "    'comp.graphics',\n", "    'sci.space',\n", "]\n", "# Uncomment the following to do the analysis on all the categories\n", "# categories = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Loading 20 newsgroups dataset for categories:\")\n", "print(categories)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = fetch_20newsgroups(subset='all', categories=categories,\n", "                             shuffle=True, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"%d documents\" % len(dataset.data))\n", "print(\"%d categories\" % len(dataset.target_names))\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["labels = dataset.target\n", "true_k = np.unique(labels).shape[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Extracting features from the training dataset \"\n", "      \"using a sparse vectorizer\")\n", "t0 = time()\n", "if opts.use_hashing:\n", "    if opts.use_idf:\n", "        # Perform an IDF normalization on the output of HashingVectorizer\n", "        hasher = HashingVectorizer(n_features=opts.n_features,\n", "                                   stop_words='english', alternate_sign=False,\n", "                                   norm=None)\n", "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n", "    else:\n", "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n", "                                       stop_words='english',\n", "                                       alternate_sign=False, norm='l2')\n", "else:\n", "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n", "                                 min_df=2, stop_words='english',\n", "                                 use_idf=opts.use_idf)\n", "X = vectorizer.fit_transform(dataset.data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"done in %fs\" % (time() - t0))\n", "print(\"n_samples: %d, n_features: %d\" % X.shape)\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if opts.n_components:\n", "    print(\"Performing dimensionality reduction using LSA\")\n", "    t0 = time()\n", "    # Vectorizer results are normalized, which makes KMeans behave as\n", "    # spherical k-means for better results. Since LSA/SVD results are\n", "    # not normalized, we have to redo the normalization.\n", "    svd = TruncatedSVD(opts.n_components)\n", "    normalizer = Normalizer(copy=False)\n", "    lsa = make_pipeline(svd, normalizer)\n", "    X = lsa.fit_transform(X)\n", "    print(\"done in %fs\" % (time() - t0))\n", "    explained_variance = svd.explained_variance_ratio_.sum()\n", "    print(\"Explained variance of the SVD step: {}%\".format(\n", "        int(explained_variance * 100)))\n", "    print()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Do the actual clustering"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if opts.minibatch:\n", "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n", "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n", "else:\n", "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n", "                verbose=opts.verbose)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Clustering sparse data with %s\" % km)\n", "t0 = time()\n", "km.fit(X)\n", "print(\"done in %0.3fs\" % (time() - t0))\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n", "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n", "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n", "print(\"Adjusted Rand-Index: %.3f\"\n", "      % metrics.adjusted_rand_score(labels, km.labels_))\n", "print(\"Silhouette Coefficient: %0.3f\"\n", "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not opts.use_hashing:\n", "    print(\"Top terms per cluster:\")\n", "    if opts.n_components:\n", "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n", "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n", "    else:\n", "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n", "    terms = vectorizer.get_feature_names()\n", "    for i in range(true_k):\n", "        print(\"Cluster %d:\" % i, end='')\n", "        for ind in order_centroids[i, :10]:\n", "            print(' %s' % terms[ind], end='')\n", "        print()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}