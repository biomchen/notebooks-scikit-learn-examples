{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "================================================================<br>\n", "Permutation Importance vs Random Forest Feature Importance (MDI)<br>\n", "================================================================<br>\n", "In this example, we will compare the impurity-based feature importance of<br>\n", ":class:`~sklearn.ensemble.RandomForestClassifier` with the<br>\n", "permutation importance on the titanic dataset using<br>\n", ":func:`~sklearn.inspection.permutation_importance`. We will show that the<br>\n", "impurity-based feature importance can inflate the importance of numerical<br>\n", "features.<br>\n", "Furthermore, the impurity-based feature importance of random forests suffers<br>\n", "from being computed on statistics derived from the training dataset: the<br>\n", "importances can be high even for features that are not predictive of the target<br>\n", "variable, as long as the model has the capacity to use them to overfit.<br>\n", "This example shows how to use Permutation Importances as an alternative that<br>\n", "can mitigate those limitations.<br>\n", ".. topic:: References:<br>\n", "   [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,<br>\n", "       2001. https://doi.org/10.1023/A:1010933404324<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)\n", "import matplotlib.pyplot as plt\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import fetch_openml\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.impute import SimpleImputer\n", "from sklearn.inspection import permutation_importance\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import OneHotEncoder"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Data Loading and Feature Engineering<br>\n", "------------------------------------<br>\n", "Let's use pandas to load a copy of the titanic dataset. The following shows<br>\n", "how to apply separate preprocessing on numerical and categorical features.<br>\n", "<br>\n", "We further include two random variables that are not correlated in any way<br>\n", "with the target variable (``survived``):<br>\n", "<br>\n", "- ``random_num`` is a high cardinality numerical variable (as many unique<br>\n", "  values as records).<br>\n", "- ``random_cat`` is a low cardinality categorical variable (3 possible<br>\n", "  values)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n", "rng = np.random.RandomState(seed=42)\n", "X['random_cat'] = rng.randint(3, size=X.shape[0])\n", "X['random_num'] = rng.randn(X.shape[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["categorical_columns = ['pclass', 'sex', 'embarked', 'random_cat']\n", "numerical_columns = ['age', 'sibsp', 'parch', 'fare', 'random_num']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = X[categorical_columns + numerical_columns]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, stratify=y, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["categorical_pipe = Pipeline([\n", "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n", "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n", "])\n", "numerical_pipe = Pipeline([\n", "    ('imputer', SimpleImputer(strategy='mean'))\n", "])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["preprocessing = ColumnTransformer(\n", "    [('cat', categorical_pipe, categorical_columns),\n", "     ('num', numerical_pipe, numerical_columns)])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rf = Pipeline([\n", "    ('preprocess', preprocessing),\n", "    ('classifier', RandomForestClassifier(random_state=42))\n", "])\n", "rf.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Accuracy of the Model<br>\n", "---------------------<br>\n", "Prior to inspecting the feature importances, it is important to check that<br>\n", "the model predictive performance is high enough. Indeed there would be little<br>\n", "interest of inspecting the important features of a non-predictive model.<br>\n", "<br>\n", "Here one can observe that the train accuracy is very high (the forest model<br>\n", "has enough capacity to completely memorize the training set) but it can still<br>\n", "generalize well enough to the test set thanks to the built-in bagging of<br>\n", "random forests.<br>\n", "<br>\n", "It might be possible to trade some accuracy on the training set for a<br>\n", "slightly better accuracy on the test set by limiting the capacity of the<br>\n", "trees (for instance by setting ``min_samples_leaf=5`` or<br>\n", "``min_samples_leaf=10``) so as to limit overfitting while not introducing too<br>\n", "much underfitting.<br>\n", "<br>\n", "However let's keep our high capacity random forest model for now so as to<br>\n", "illustrate some pitfalls with feature importance on variables with many<br>\n", "unique values."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"RF train accuracy: %0.3f\" % rf.score(X_train, y_train))\n", "print(\"RF test accuracy: %0.3f\" % rf.score(X_test, y_test))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Tree's Feature Importance from Mean Decrease in Impurity (MDI)<br>\n", "--------------------------------------------------------------<br>\n", "The impurity-based feature importance ranks the numerical features to be the<br>\n", "most important features. As a result, the non-predictive ``random_num``<br>\n", "variable is ranked the most important!<br>\n", "<br>\n", "This problem stems from two limitations of impurity-based feature<br>\n", "importances:<br>\n", "<br>\n", "- impurity-based importances are biased towards high cardinality features;<br>\n", "- impurity-based importances are computed on training set statistics and<br>\n", "  therefore do not reflect the ability of feature to be useful to make<br>\n", "  predictions that generalize to the test set (when the model has enough<br>\n", "  capacity)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ohe = (rf.named_steps['preprocess']\n", "         .named_transformers_['cat']\n", "         .named_steps['onehot'])\n", "feature_names = ohe.get_feature_names(input_features=categorical_columns)\n", "feature_names = np.r_[feature_names, numerical_columns]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tree_feature_importances = (\n", "    rf.named_steps['classifier'].feature_importances_)\n", "sorted_idx = tree_feature_importances.argsort()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_ticks = np.arange(0, len(feature_names))\n", "fig, ax = plt.subplots()\n", "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n", "ax.set_yticklabels(feature_names[sorted_idx])\n", "ax.set_yticks(y_ticks)\n", "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n", "fig.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "As an alternative, the permutation importances of ``rf`` are computed on a<br>\n", "held out test set. This shows that the low cardinality categorical feature,<br>\n", "``sex`` is the most important feature.<br>\n", "<br>\n", "Also note that both random features have very low importances (close to 0) as<br>\n", "expected."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result = permutation_importance(rf, X_test, y_test, n_repeats=10,\n", "                                random_state=42, n_jobs=2)\n", "sorted_idx = result.importances_mean.argsort()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.boxplot(result.importances[sorted_idx].T,\n", "           vert=False, labels=X_test.columns[sorted_idx])\n", "ax.set_title(\"Permutation Importances (test set)\")\n", "fig.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "It is also possible to compute the permutation importances on the training<br>\n", "set. This reveals that ``random_num`` gets a significantly higher importance<br>\n", "ranking than when computed on the test set. The difference between those two<br>\n", "plots is a confirmation that the RF model has enough capacity to use that<br>\n", "random numerical feature to overfit. You can further confirm this by<br>\n", "re-running this example with constrained RF with min_samples_leaf=10."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result = permutation_importance(rf, X_train, y_train, n_repeats=10,\n", "                                random_state=42, n_jobs=2)\n", "sorted_idx = result.importances_mean.argsort()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.boxplot(result.importances[sorted_idx].T,\n", "           vert=False, labels=X_train.columns[sorted_idx])\n", "ax.set_title(\"Permutation Importances (train set)\")\n", "fig.tight_layout()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}