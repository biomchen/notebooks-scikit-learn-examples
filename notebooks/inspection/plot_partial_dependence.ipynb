{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "========================<br>\n", "Partial Dependence Plots<br>\n", "========================<br>\n", "Partial dependence plots show the dependence between the target function [2]_<br>\n", "and a set of 'target' features, marginalizing over the values of all other<br>\n", "features (the complement features). Due to the limits of human perception, the<br>\n", "size of the target feature set must be small (usually, one or two) thus the<br>\n", "target features are usually chosen among the most important features.<br>\n", "This example shows how to obtain partial dependence plots from a<br>\n", ":class:`~sklearn.neural_network.MLPRegressor` and a<br>\n", ":class:`~sklearn.ensemble.HistGradientBoostingRegressor` trained on the<br>\n", "California housing dataset. The example is taken from [1]_.<br>\n", "The plots show four 1-way and two 1-way partial dependence plots (omitted for<br>\n", ":class:`~sklearn.neural_network.MLPRegressor` due to computation time). The<br>\n", "target variables for the one-way PDP are: median income (`MedInc`), average<br>\n", "occupants per household (`AvgOccup`), median house age (`HouseAge`), and<br>\n", "average rooms per household (`AveRooms`).<br>\n", ".. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical<br>\n", "       Learning Ed. 2\", Springer, 2009.<br>\n", ".. [2] For classification you can think of it as the regression score before<br>\n", "       the link function.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from time import time\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from mpl_toolkits.mplot3d import Axes3D"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import QuantileTransformer\n", "from sklearn.pipeline import make_pipeline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.inspection import partial_dependence\n", "from sklearn.inspection import plot_partial_dependence\n", "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n", "from sklearn.ensemble import HistGradientBoostingRegressor\n", "from sklearn.neural_network import MLPRegressor\n", "from sklearn.datasets import fetch_california_housing"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "California Housing data preprocessing<br>\n", "-------------------------------------<br>\n", "<br>\n", "Center target to avoid gradient boosting init bias: gradient boosting<br>\n", "with the 'recursion' method does not account for the initial estimator<br>\n", "(here the average target, by default)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cal_housing = fetch_california_housing()\n", "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n", "y = cal_housing.target"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y -= y.mean()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n", "                                                    random_state=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Partial Dependence computation for multi-layer perceptron<br>\n", "---------------------------------------------------------<br>\n", "<br>\n", "Let's fit a MLPRegressor and compute single-variable partial dependence<br>\n", "plots"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Training MLPRegressor...\")\n", "tic = time()\n", "est = make_pipeline(QuantileTransformer(),\n", "                    MLPRegressor(hidden_layer_sizes=(50, 50),\n", "                                 learning_rate_init=0.01,\n", "                                 early_stopping=True))\n", "est.fit(X_train, y_train)\n", "print(\"done in {:.3f}s\".format(time() - tic))\n", "print(\"Test R2 score: {:.2f}\".format(est.score(X_test, y_test)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "We configured a pipeline to scale the numerical input features and tuned the<br>\n", "neural network size and learning rate to get a reasonable compromise between<br>\n", "training time and predictive performance on a test set.<br>\n", "<br>\n", "Importantly, this tabular dataset has very different dynamic ranges for its<br>\n", "features. Neural networks tend to be very sensitive to features with varying<br>\n", "scales and forgetting to preprocess the numeric feature would lead to a very<br>\n", "poor model.<br>\n", "<br>\n", "It would be possible to get even higher predictive performance with a larger<br>\n", "neural network but the training would also be significantly more expensive.<br>\n", "<br>\n", "Note that it is important to check that the model is accurate enough on a<br>\n", "test set before plotting the partial dependence since there would be little<br>\n", "use in explaining the impact of a given feature on the prediction function of<br>\n", "a poor model.<br>\n", "<br>\n", "Let's now compute the partial dependence plots for this neural network using<br>\n", "the model-agnostic (brute-force) method:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Computing partial dependence plots...')\n", "tic = time()\n", "# We don't compute the 2-way PDP (5, 1) here, because it is a lot slower\n", "# with the brute method.\n", "features = ['MedInc', 'AveOccup', 'HouseAge', 'AveRooms']\n", "plot_partial_dependence(est, X_train, features,\n", "                        n_jobs=3, grid_resolution=20)\n", "print(\"done in {:.3f}s\".format(time() - tic))\n", "fig = plt.gcf()\n", "fig.suptitle('Partial dependence of house value on non-location features\\n'\n", "             'for the California housing dataset, with MLPRegressor')\n", "fig.subplots_adjust(hspace=0.3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Partial Dependence computation for Gradient Boosting<br>\n", "----------------------------------------------------<br>\n", "<br>\n", "Let's now fit a GradientBoostingRegressor and compute the partial dependence<br>\n", "plots either or one or two variables at a time."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Training GradientBoostingRegressor...\")\n", "tic = time()\n", "est = HistGradientBoostingRegressor()\n", "est.fit(X_train, y_train)\n", "print(\"done in {:.3f}s\".format(time() - tic))\n", "print(\"Test R2 score: {:.2f}\".format(est.score(X_test, y_test)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Here, we used the default hyperparameters for the gradient boosting model<br>\n", "without any preprocessing as tree-based models are naturally robust to<br>\n", "monotonic transformations of numerical features.<br>\n", "<br>\n", "Note that on this tabular dataset, Gradient Boosting Machines are both<br>\n", "significantly faster to train and more accurate than neural networks. It is<br>\n", "also significantly cheaper to tune their hyperparameters (the default tend to<br>\n", "work well while this is not often the case for neural networks).<br>\n", "<br>\n", "Finally, as we will see next, computing partial dependence plots tree-based<br>\n", "models is also orders of magnitude faster making it cheap to compute partial<br>\n", "dependence plots for pairs of interacting features:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Computing partial dependence plots...')\n", "tic = time()\n", "features = ['MedInc', 'AveOccup', 'HouseAge', 'AveRooms',\n", "            ('AveOccup', 'HouseAge')]\n", "plot_partial_dependence(est, X_train, features,\n", "                        n_jobs=3, grid_resolution=20)\n", "print(\"done in {:.3f}s\".format(time() - tic))\n", "fig = plt.gcf()\n", "fig.suptitle('Partial dependence of house value on non-location features\\n'\n", "             'for the California housing dataset, with Gradient Boosting')\n", "fig.subplots_adjust(wspace=0.4, hspace=0.3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Analysis of the plots<br>\n", "---------------------<br>\n", "<br>\n", "We can clearly see that the median house price shows a linear relationship<br>\n", "with the median income (top left) and that the house price drops when the<br>\n", "average occupants per household increases (top middle).<br>\n", "The top right plot shows that the house age in a district does not have<br>\n", "a strong influence on the (median) house price; so does the average rooms<br>\n", "per household.<br>\n", "The tick marks on the x-axis represent the deciles of the feature values<br>\n", "in the training data.<br>\n", "<br>\n", "We also observe that :class:`~sklearn.neural_network.MLPRegressor` has much<br>\n", "smoother predictions than<br>\n", ":class:`~sklearn.ensemble.HistGradientBoostingRegressor`. For the plots to be<br>\n", "comparable, it is necessary to subtract the average value of the target<br>\n", "``y``: The 'recursion' method, used by default for<br>\n", ":class:`~sklearn.ensemble.HistGradientBoostingRegressor`, does not account<br>\n", "for the initial predictor (in our case the average target). Setting the<br>\n", "target average to 0 avoids this bias.<br>\n", "<br>\n", "Partial dependence plots with two target features enable us to visualize<br>\n", "interactions among them. The two-way partial dependence plot shows the<br>\n", "dependence of median house price on joint values of house age and average<br>\n", "occupants per household. We can clearly see an interaction between the<br>\n", "two features: for an average occupancy greater than two, the house price is<br>\n", "nearly independent of the house age, whereas for values less than two there<br>\n", "is a strong dependence on age."]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "3D interaction plots<br>\n", "--------------------<br>\n", "<br>\n", "Let's make the same partial dependence plot for the 2 features interaction,<br>\n", "this time in 3 dimensions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = plt.figure()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["features = ('AveOccup', 'HouseAge')\n", "pdp, axes = partial_dependence(est, X_train, features=features,\n", "                               grid_resolution=20)\n", "XX, YY = np.meshgrid(axes[0], axes[1])\n", "Z = pdp[0].T\n", "ax = Axes3D(fig)\n", "surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,\n", "                       cmap=plt.cm.BuPu, edgecolor='k')\n", "ax.set_xlabel(features[0])\n", "ax.set_ylabel(features[1])\n", "ax.set_zlabel('Partial dependence')\n", "#  pretty init view\n", "ax.view_init(elev=22, azim=122)\n", "plt.colorbar(surf)\n", "plt.suptitle('Partial dependence of house value on median\\n'\n", "             'age and average occupancy, with Gradient Boosting')\n", "plt.subplots_adjust(top=0.9)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}