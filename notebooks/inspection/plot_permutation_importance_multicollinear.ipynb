{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "=================================================================<br>\n", "Permutation Importance with Multicollinear or Correlated Features<br>\n", "=================================================================<br>\n", "In this example, we compute the permutation importance on the Wisconsin<br>\n", "breast cancer dataset using :func:`~sklearn.inspection.permutation_importance`.<br>\n", "The :class:`~sklearn.ensemble.RandomForestClassifier` can easily get about 97%<br>\n", "accuracy on a test dataset. Because this dataset contains multicollinear<br>\n", "features, the permutation importance will show that none of the features are<br>\n", "important. One approach to handling multicollinearity is by performing<br>\n", "hierarchical clustering on the features' Spearman rank-order correlations,<br>\n", "picking a threshold, and keeping a single feature from each cluster.<br>\n", ".. note::<br>\n", "    See also<br>\n", "    :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)\n", "from collections import defaultdict"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np\n", "from scipy.stats import spearmanr\n", "from scipy.cluster import hierarchy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_breast_cancer\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.inspection import permutation_importance\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Random Forest Feature Importance on Breast Cancer Data<br>\n", "------------------------------------------------------<br>\n", "First, we train a random forest on the breast cancer dataset and evaluate<br>\n", "its accuracy on a test set:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = load_breast_cancer()\n", "X, y = data.data, data.target\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = RandomForestClassifier(n_estimators=100, random_state=42)\n", "clf.fit(X_train, y_train)\n", "print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Next, we plot the tree based feature importance and the permutation<br>\n", "importance. The permutation importance plot shows that permuting a feature<br>\n", "drops the accuracy by at most `0.012`, which would suggest that none of the<br>\n", "features are important. This is in contradiction with the high test accuracy<br>\n", "computed above: some feature must be important. The permutation importance<br>\n", "is calculated on the training set to show how much the model relies on each<br>\n", "feature during training."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result = permutation_importance(clf, X_train, y_train, n_repeats=10,\n", "                                random_state=42)\n", "perm_sorted_idx = result.importances_mean.argsort()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n", "tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n", "ax1.barh(tree_indices,\n", "         clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n", "ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])\n", "ax1.set_yticks(tree_indices)\n", "ax1.set_ylim((0, len(clf.feature_importances_)))\n", "ax2.boxplot(result.importances[perm_sorted_idx].T, vert=False,\n", "            labels=data.feature_names[perm_sorted_idx])\n", "fig.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Handling Multicollinear Features<br>\n", "--------------------------------<br>\n", "When features are collinear, permutating one feature will have little<br>\n", "effect on the models performance because it can get the same information<br>\n", "from a correlated feature. One way to handle multicollinear features is by<br>\n", "performing hierarchical clustering on the Spearman rank-order correlations,<br>\n", "picking a threshold, and keeping a single feature from each cluster. First,<br>\n", "we plot a heatmap of the correlated features:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n", "corr = spearmanr(X).correlation\n", "corr_linkage = hierarchy.ward(corr)\n", "dendro = hierarchy.dendrogram(corr_linkage, labels=data.feature_names, ax=ax1,\n", "                              leaf_rotation=90)\n", "dendro_idx = np.arange(0, len(dendro['ivl']))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']])\n", "ax2.set_xticks(dendro_idx)\n", "ax2.set_yticks(dendro_idx)\n", "ax2.set_xticklabels(dendro['ivl'], rotation='vertical')\n", "ax2.set_yticklabels(dendro['ivl'])\n", "fig.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Next, we manually pick a threshold by visual inspection of the dendrogram<br>\n", "to group our features into clusters and choose a feature from each cluster to<br>\n", "keep, select those features from our dataset, and train a new random forest.<br>\n", "The test accuracy of the new random forest did not change much compared to<br>\n", "the random forest trained on the complete dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cluster_ids = hierarchy.fcluster(corr_linkage, 1, criterion='distance')\n", "cluster_id_to_feature_ids = defaultdict(list)\n", "for idx, cluster_id in enumerate(cluster_ids):\n", "    cluster_id_to_feature_ids[cluster_id].append(idx)\n", "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train_sel = X_train[:, selected_features]\n", "X_test_sel = X_test[:, selected_features]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_sel = RandomForestClassifier(n_estimators=100, random_state=42)\n", "clf_sel.fit(X_train_sel, y_train)\n", "print(\"Accuracy on test data with features removed: {:.2f}\".format(\n", "      clf_sel.score(X_test_sel, y_test)))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}