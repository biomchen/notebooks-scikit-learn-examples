{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/python\n", "# -*- coding: utf-8 -*-\n", "\"\"\"\n", "======================\n", "Feature discretization\n", "======================"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["A demonstration of feature discretization on synthetic classification datasets.\n", "Feature discretization decomposes each feature into a set of bins, here equally\n", "distributed in width. The discrete values are then one-hot encoded, and given\n", "to a linear classifier. This preprocessing enables a non-linear behavior even\n", "though the classifier is linear."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["On this example, the first two rows represent linearly non-separable datasets\n", "(moons and concentric circles) while the third is approximately linearly\n", "separable. On the two linearly non-separable datasets, feature discretization\n", "largely increases the performance of linear classifiers. On the linearly\n", "separable dataset, feature discretization decreases the performance of linear\n", "classifiers. Two non-linear classifiers are also shown for comparison."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["This example should be taken with a grain of salt, as the intuition conveyed\n", "does not necessarily carry over to real datasets. Particularly in\n", "high-dimensional spaces, data can more easily be separated linearly. Moreover,\n", "using feature discretization and one-hot encoding increases the number of\n", "features, which easily lead to overfitting when the number of samples is small."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["The plots show training points in solid colors and testing points\n", "semi-transparent. The lower right shows the classification accuracy on the test\n", "set.\n", "\"\"\"\n", "# Code source: Tom Dupr\u00e9 la Tour\n", "# Adapted from plot_classifier_comparison by Ga\u00ebl Varoquaux and Andreas M\u00fcller\n", "#\n", "# License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from matplotlib.colors import ListedColormap\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.datasets import make_moons, make_circles, make_classification\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.pipeline import make_pipeline\n", "from sklearn.preprocessing import KBinsDiscretizer\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.utils._testing import ignore_warnings\n", "from sklearn.exceptions import ConvergenceWarning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["h = .02  # step size in the mesh"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_name(estimator):\n", "    name = estimator.__class__.__name__\n", "    if name == 'Pipeline':\n", "        name = [get_name(est[1]) for est in estimator.steps]\n", "        name = ' + '.join(name)\n", "    return name"]}, {"cell_type": "markdown", "metadata": {}, "source": ["list of (estimator, param_grid), where param_grid is used in GridSearchCV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["classifiers = [\n", "    (LogisticRegression(random_state=0), {\n", "        'C': np.logspace(-2, 7, 10)\n", "    }),\n", "    (LinearSVC(random_state=0), {\n", "        'C': np.logspace(-2, 7, 10)\n", "    }),\n", "    (make_pipeline(\n", "        KBinsDiscretizer(encode='onehot'),\n", "        LogisticRegression(random_state=0)), {\n", "            'kbinsdiscretizer__n_bins': np.arange(2, 10),\n", "            'logisticregression__C': np.logspace(-2, 7, 10),\n", "        }),\n", "    (make_pipeline(\n", "        KBinsDiscretizer(encode='onehot'), LinearSVC(random_state=0)), {\n", "            'kbinsdiscretizer__n_bins': np.arange(2, 10),\n", "            'linearsvc__C': np.logspace(-2, 7, 10),\n", "        }),\n", "    (GradientBoostingClassifier(n_estimators=50, random_state=0), {\n", "        'learning_rate': np.logspace(-4, 0, 10)\n", "    }),\n", "    (SVC(random_state=0), {\n", "        'C': np.logspace(-2, 7, 10)\n", "    }),\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["names = [get_name(e) for e, g in classifiers]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples = 100\n", "datasets = [\n", "    make_moons(n_samples=n_samples, noise=0.2, random_state=0),\n", "    make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),\n", "    make_classification(n_samples=n_samples, n_features=2, n_redundant=0,\n", "                        n_informative=2, random_state=2,\n", "                        n_clusters_per_class=1)\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(nrows=len(datasets), ncols=len(classifiers) + 1,\n", "                         figsize=(21, 9))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cm = plt.cm.PiYG\n", "cm_bright = ListedColormap(['#b30065', '#178000'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["iterate over datasets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ds_cnt, (X, y) in enumerate(datasets):\n", "    print('\\ndataset %d\\n---------' % ds_cnt)\n\n", "    # preprocess dataset, split into training and test part\n", "    X = StandardScaler().fit_transform(X)\n", "    X_train, X_test, y_train, y_test = train_test_split(\n", "        X, y, test_size=.5, random_state=42)\n\n", "    # create the grid for background colors\n", "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", "    xx, yy = np.meshgrid(\n", "        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n", "    # plot the dataset first\n", "    ax = axes[ds_cnt, 0]\n", "    if ds_cnt == 0:\n", "        ax.set_title(\"Input data\")\n", "    # plot the training points\n", "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n", "               edgecolors='k')\n", "    # and testing points\n", "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n", "               edgecolors='k')\n", "    ax.set_xlim(xx.min(), xx.max())\n", "    ax.set_ylim(yy.min(), yy.max())\n", "    ax.set_xticks(())\n", "    ax.set_yticks(())\n\n", "    # iterate over classifiers\n", "    for est_idx, (name, (estimator, param_grid)) in \\\n", "            enumerate(zip(names, classifiers)):\n", "        ax = axes[ds_cnt, est_idx + 1]\n", "        clf = GridSearchCV(estimator=estimator, param_grid=param_grid)\n", "        with ignore_warnings(category=ConvergenceWarning):\n", "            clf.fit(X_train, y_train)\n", "        score = clf.score(X_test, y_test)\n", "        print('%s: %.2f' % (name, score))\n\n", "        # plot the decision boundary. For that, we will assign a color to each\n", "        # point in the mesh [x_min, x_max]*[y_min, y_max].\n", "        if hasattr(clf, \"decision_function\"):\n", "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n", "        else:\n", "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n", "        # put the result into a color plot\n", "        Z = Z.reshape(xx.shape)\n", "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n", "        # plot the training points\n", "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n", "                   edgecolors='k')\n", "        # and testing points\n", "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n", "                   edgecolors='k', alpha=0.6)\n", "        ax.set_xlim(xx.min(), xx.max())\n", "        ax.set_ylim(yy.min(), yy.max())\n", "        ax.set_xticks(())\n", "        ax.set_yticks(())\n", "        if ds_cnt == 0:\n", "            ax.set_title(name.replace(' + ', '\\n'))\n", "        ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0'), size=15,\n", "                bbox=dict(boxstyle='round', alpha=0.8, facecolor='white'),\n", "                transform=ax.transAxes, horizontalalignment='right')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.tight_layout()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Add suptitles above the figure"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplots_adjust(top=0.90)\n", "suptitles = [\n", "    'Linear classifiers',\n", "    'Feature discretization and linear classifiers',\n", "    'Non-linear classifiers',\n", "]\n", "for i, suptitle in zip([1, 3, 5], suptitles):\n", "    ax = axes[0, i]\n", "    ax.text(1.05, 1.25, suptitle, transform=ax.transAxes,\n", "            horizontalalignment='center', size='x-large')\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}