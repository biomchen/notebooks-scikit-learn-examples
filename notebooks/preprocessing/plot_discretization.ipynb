{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "================================================================<br>\n", "Using KBinsDiscretizer to discretize continuous features<br>\n", "================================================================<br>\n", "The example compares prediction result of linear regression (linear model)<br>\n", "and decision tree (tree based model) with and without discretization of<br>\n", "real-valued features.<br>\n", "As is shown in the result before discretization, linear model is fast to<br>\n", "build and relatively straightforward to interpret, but can only model<br>\n", "linear relationships, while decision tree can build a much more complex model<br>\n", "of the data. One way to make linear model more powerful on continuous data<br>\n", "is to use discretization (also known as binning). In the example, we<br>\n", "discretize the feature and one-hot encode the transformed data. Note that if<br>\n", "the bins are not reasonably wide, there would appear to be a substantially<br>\n", "increased risk of overfitting, so the discretizer parameters should usually<br>\n", "be tuned under cross validation.<br>\n", "After discretization, linear regression and decision tree make exactly the<br>\n", "same prediction. As features are constant within each bin, any model must<br>\n", "predict the same value for all points within a bin. Compared with the result<br>\n", "before discretization, linear model become much more flexible while decision<br>\n", "tree gets much less flexible. Note that binning features generally has no<br>\n", "beneficial effect for tree-based models, as these models can learn to split<br>\n", "up the data anywhere.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Andreas M\u00fcller<br>\n", "        Hanmin Qin <qinhanmin2005@sina.com><br>\n", "License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "from sklearn.preprocessing import KBinsDiscretizer\n", "from sklearn.tree import DecisionTreeRegressor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["construct the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rnd = np.random.RandomState(42)\n", "X = rnd.uniform(-3, 3, size=100)\n", "y = np.sin(X) + rnd.normal(size=len(X)) / 3\n", "X = X.reshape(-1, 1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["transform the dataset with KBinsDiscretizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["enc = KBinsDiscretizer(n_bins=10, encode='onehot')\n", "X_binned = enc.fit_transform(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["predict with original dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))\n", "line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\n", "reg = LinearRegression().fit(X, y)\n", "ax1.plot(line, reg.predict(line), linewidth=2, color='green',\n", "         label=\"linear regression\")\n", "reg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X, y)\n", "ax1.plot(line, reg.predict(line), linewidth=2, color='red',\n", "         label=\"decision tree\")\n", "ax1.plot(X[:, 0], y, 'o', c='k')\n", "ax1.legend(loc=\"best\")\n", "ax1.set_ylabel(\"Regression output\")\n", "ax1.set_xlabel(\"Input feature\")\n", "ax1.set_title(\"Result before discretization\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["predict with transformed dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["line_binned = enc.transform(line)\n", "reg = LinearRegression().fit(X_binned, y)\n", "ax2.plot(line, reg.predict(line_binned), linewidth=2, color='green',\n", "         linestyle='-', label='linear regression')\n", "reg = DecisionTreeRegressor(min_samples_split=3,\n", "                            random_state=0).fit(X_binned, y)\n", "ax2.plot(line, reg.predict(line_binned), linewidth=2, color='red',\n", "         linestyle=':', label='decision tree')\n", "ax2.plot(X[:, 0], y, 'o', c='k')\n", "ax2.vlines(enc.bin_edges_[0], *plt.gca().get_ylim(), linewidth=1, alpha=.2)\n", "ax2.legend(loc=\"best\")\n", "ax2.set_xlabel(\"Input feature\")\n", "ax2.set_title(\"Result after discretization\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.tight_layout()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}