{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "=================================<br>\n", "Map data to a normal distribution<br>\n", "=================================<br>\n", ".. currentmodule:: sklearn.preprocessing<br>\n", "This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms<br>\n", "through :class:`~PowerTransformer` to map data from various<br>\n", "distributions to a normal distribution.<br>\n", "The power transform is useful as a transformation in modeling problems where<br>\n", "homoscedasticity and normality are desired. Below are examples of Box-Cox and<br>\n", "Yeo-Johnwon applied to six different probability distributions: Lognormal,<br>\n", "Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.<br>\n", "Note that the transformations successfully map the data to a normal<br>\n", "distribution when applied to certain datasets, but are ineffective with others.<br>\n", "This highlights the importance of visualizing the data before and after<br>\n", "transformation.<br>\n", "Also note that even though Box-Cox seems to perform better than Yeo-Johnson for<br>\n", "lognormal and chi-squared distributions, keep in mind that Box-Cox does not<br>\n", "support inputs with negative values.<br>\n", "For comparison, we also add the output from<br>\n", ":class:`~QuantileTransformer`. It can force any arbitrary<br>\n", "distribution into a gaussian, provided that there are enough training samples<br>\n", "(thousands). Because it is a non-parametric method, it is harder to interpret<br>\n", "than the parametric ones (Box-Cox and Yeo-Johnson).<br>\n", "On \"small\" datasets (less than a few hundred points), the quantile transformer<br>\n", "is prone to overfitting. The use of the power transform is then recommended.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Eric Chang <ericchang2017@u.northwestern.edu><br>\n", "        Nicolas Hug <contact@nicolas-hug.com><br>\n", "License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import PowerTransformer\n", "from sklearn.preprocessing import QuantileTransformer\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["N_SAMPLES = 1000\n", "FONT_SIZE = 6\n", "BINS = 30"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rng = np.random.RandomState(304)\n", "bc = PowerTransformer(method='box-cox')\n", "yj = PowerTransformer(method='yeo-johnson')\n", "# n_quantiles is set to the training set size rather than the default value\n", "# to avoid a warning being raised by this example\n", "qt = QuantileTransformer(n_quantiles=500, output_distribution='normal',\n", "                         random_state=rng)\n", "size = (N_SAMPLES, 1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["lognormal distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_lognormal = rng.lognormal(size=size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["chi-squared distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = 3\n", "X_chisq = rng.chisquare(df=df, size=size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["weibull distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a = 50\n", "X_weibull = rng.weibull(a=a, size=size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["gaussian distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loc = 100\n", "X_gaussian = rng.normal(loc=loc, size=size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["uniform distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_uniform = rng.uniform(low=0, high=1, size=size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["bimodal distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loc_a, loc_b = 100, 105\n", "X_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\n", "X_bimodal = np.concatenate([X_a, X_b], axis=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["create plots"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["distributions = [\n", "    ('Lognormal', X_lognormal),\n", "    ('Chi-squared', X_chisq),\n", "    ('Weibull', X_weibull),\n", "    ('Gaussian', X_gaussian),\n", "    ('Uniform', X_uniform),\n", "    ('Bimodal', X_bimodal)\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["colors = ['#D81B60', '#0188FF', '#FFC107',\n", "          '#B7A2FF', '#000000', '#2EC5AC']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(nrows=8, ncols=3, figsize=plt.figaspect(2))\n", "axes = axes.flatten()\n", "axes_idxs = [(0, 3, 6, 9), (1, 4, 7, 10), (2, 5, 8, 11), (12, 15, 18, 21),\n", "             (13, 16, 19, 22), (14, 17, 20, 23)]\n", "axes_list = [(axes[i], axes[j], axes[k], axes[l])\n", "             for (i, j, k, l) in axes_idxs]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for distribution, color, axes in zip(distributions, colors, axes_list):\n", "    name, X = distribution\n", "    X_train, X_test = train_test_split(X, test_size=.5)\n\n", "    # perform power transforms and quantile transform\n", "    X_trans_bc = bc.fit(X_train).transform(X_test)\n", "    lmbda_bc = round(bc.lambdas_[0], 2)\n", "    X_trans_yj = yj.fit(X_train).transform(X_test)\n", "    lmbda_yj = round(yj.lambdas_[0], 2)\n", "    X_trans_qt = qt.fit(X_train).transform(X_test)\n", "    ax_original, ax_bc, ax_yj, ax_qt = axes\n", "    ax_original.hist(X_train, color=color, bins=BINS)\n", "    ax_original.set_title(name, fontsize=FONT_SIZE)\n", "    ax_original.tick_params(axis='both', which='major', labelsize=FONT_SIZE)\n", "    for ax, X_trans, meth_name, lmbda in zip(\n", "            (ax_bc, ax_yj, ax_qt),\n", "            (X_trans_bc, X_trans_yj, X_trans_qt),\n", "            ('Box-Cox', 'Yeo-Johnson', 'Quantile transform'),\n", "            (lmbda_bc, lmbda_yj, None)):\n", "        ax.hist(X_trans, color=color, bins=BINS)\n", "        title = 'After {}'.format(meth_name)\n", "        if lmbda is not None:\n", "            title += r'\\n$\\lambda$ = {}'.format(lmbda)\n", "        ax.set_title(title, fontsize=FONT_SIZE)\n", "        ax.tick_params(axis='both', which='major', labelsize=FONT_SIZE)\n", "        ax.set_xlim([-3.5, 3.5])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.tight_layout()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}