{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "======================================<br>\n", "Probability calibration of classifiers<br>\n", "======================================<br>\n", "When performing classification you often want to predict not only<br>\n", "the class label, but also the associated probability. This probability<br>\n", "gives you some kind of confidence on the prediction. However, not all<br>\n", "classifiers provide well-calibrated probabilities, some being over-confident<br>\n", "while others being under-confident. Thus, a separate calibration of predicted<br>\n", "probabilities is often desirable as a postprocessing. This example illustrates<br>\n", "two different methods for this calibration and evaluates the quality of the<br>\n", "returned probabilities using Brier's score<br>\n", "(see https://en.wikipedia.org/wiki/Brier_score).<br>\n", "Compared are the estimated probability using a Gaussian naive Bayes classifier<br>\n", "without calibration, with a sigmoid calibration, and with a non-parametric<br>\n", "isotonic calibration. One can observe that only the non-parametric model is<br>\n", "able to provide a probability calibration that returns probabilities close<br>\n", "to the expected 0.5 for most of the samples belonging to the middle<br>\n", "cluster with heterogeneous labels. This results in a significantly improved<br>\n", "Brier score.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Mathieu Blondel <mathieu@mblondel.org><br>\n", "        Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr><br>\n", "        Balazs Kegl <balazs.kegl@gmail.com><br>\n", "        Jan Hendrik Metzen <jhm@informatik.uni-bremen.de><br>\n", "License: BSD Style."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from matplotlib import cm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_blobs\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.metrics import brier_score_loss\n", "from sklearn.calibration import CalibratedClassifierCV\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples = 50000\n", "n_bins = 3  # use 3 bins for calibration_curve as we have 3 clusters here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Generate 3 blobs with 2 classes where the second blob contains<br>\n", "half positive samples and half negative samples. Probability in this<br>\n", "blob is therefore 0.5."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["centers = [(-5, -5), (0, 0), (5, 5)]\n", "X, y = make_blobs(n_samples=n_samples, centers=centers, shuffle=False,\n", "                  random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y[:n_samples // 2] = 0\n", "y[n_samples // 2:] = 1\n", "sample_weight = np.random.RandomState(42).rand(y.shape[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["split train, test for calibration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test, sw_train, sw_test = \\\n", "    train_test_split(X, y, sample_weight, test_size=0.9, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gaussian Naive-Bayes with no calibration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = GaussianNB()\n", "clf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\n", "prob_pos_clf = clf.predict_proba(X_test)[:, 1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gaussian Naive-Bayes with isotonic calibration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')\n", "clf_isotonic.fit(X_train, y_train, sw_train)\n", "prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gaussian Naive-Bayes with sigmoid calibration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid')\n", "clf_sigmoid.fit(X_train, y_train, sw_train)\n", "prob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Brier scores: (the smaller the better)\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_score = brier_score_loss(y_test, prob_pos_clf, sw_test)\n", "print(\"No calibration: %1.3f\" % clf_score)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sw_test)\n", "print(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sw_test)\n", "print(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Plot the data and the predicted probabilities"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "y_unique = np.unique(y)\n", "colors = cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\n", "for this_y, color in zip(y_unique, colors):\n", "    this_X = X_train[y_train == this_y]\n", "    this_sw = sw_train[y_train == this_y]\n", "    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50,\n", "                c=color[np.newaxis, :],\n", "                alpha=0.5, edgecolor='k',\n", "                label=\"Class %s\" % this_y)\n", "plt.legend(loc=\"best\")\n", "plt.title(\"Data\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "order = np.lexsort((prob_pos_clf, ))\n", "plt.plot(prob_pos_clf[order], 'r', label='No calibration (%1.3f)' % clf_score)\n", "plt.plot(prob_pos_isotonic[order], 'g', linewidth=3,\n", "         label='Isotonic calibration (%1.3f)' % clf_isotonic_score)\n", "plt.plot(prob_pos_sigmoid[order], 'b', linewidth=3,\n", "         label='Sigmoid calibration (%1.3f)' % clf_sigmoid_score)\n", "plt.plot(np.linspace(0, y_test.size, 51)[1::2],\n", "         y_test[order].reshape(25, -1).mean(1),\n", "         'k', linewidth=3, label=r'Empirical')\n", "plt.ylim([-0.05, 1.05])\n", "plt.xlabel(\"Instances sorted according to predicted probability \"\n", "           \"(uncalibrated GNB)\")\n", "plt.ylabel(\"P(y=1)\")\n", "plt.legend(loc=\"upper left\")\n", "plt.title(\"Gaussian naive Bayes probabilities\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}