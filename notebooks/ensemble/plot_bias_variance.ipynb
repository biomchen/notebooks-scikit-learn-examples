{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "============================================================<br>\n", "Single estimator versus bagging: bias-variance decomposition<br>\n", "============================================================<br>\n", "This example illustrates and compares the bias-variance decomposition of the<br>\n", "expected mean squared error of a single estimator against a bagging ensemble.<br>\n", "In regression, the expected mean squared error of an estimator can be<br>\n", "decomposed in terms of bias, variance and noise. On average over datasets of<br>\n", "the regression problem, the bias term measures the average amount by which the<br>\n", "predictions of the estimator differ from the predictions of the best possible<br>\n", "estimator for the problem (i.e., the Bayes model). The variance term measures<br>\n", "the variability of the predictions of the estimator when fit over different<br>\n", "instances LS of the problem. Finally, the noise measures the irreducible part<br>\n", "of the error which is due the variability in the data.<br>\n", "The upper left figure illustrates the predictions (in dark red) of a single<br>\n", "decision tree trained over a random dataset LS (the blue dots) of a toy 1d<br>\n", "regression problem. It also illustrates the predictions (in light red) of other<br>\n", "single decision trees trained over other (and different) randomly drawn<br>\n", "instances LS of the problem. Intuitively, the variance term here corresponds to<br>\n", "the width of the beam of predictions (in light red) of the individual<br>\n", "estimators. The larger the variance, the more sensitive are the predictions for<br>\n", "`x` to small changes in the training set. The bias term corresponds to the<br>\n", "difference between the average prediction of the estimator (in cyan) and the<br>\n", "best possible model (in dark blue). On this problem, we can thus observe that<br>\n", "the bias is quite low (both the cyan and the blue curves are close to each<br>\n", "other) while the variance is large (the red beam is rather wide).<br>\n", "The lower left figure plots the pointwise decomposition of the expected mean<br>\n", "squared error of a single decision tree. It confirms that the bias term (in<br>\n", "blue) is low while the variance is large (in green). It also illustrates the<br>\n", "noise part of the error which, as expected, appears to be constant and around<br>\n", "`0.01`.<br>\n", "The right figures correspond to the same plots but using instead a bagging<br>\n", "ensemble of decision trees. In both figures, we can observe that the bias term<br>\n", "is larger than in the previous case. In the upper right figure, the difference<br>\n", "between the average prediction (in cyan) and the best possible model is larger<br>\n", "(e.g., notice the offset around `x=2`). In the lower right figure, the bias<br>\n", "curve is also slightly higher than in the lower left figure. In terms of<br>\n", "variance however, the beam of predictions is narrower, which suggests that the<br>\n", "variance is lower. Indeed, as the lower right figure confirms, the variance<br>\n", "term (in green) is lower than for single decision trees. Overall, the bias-<br>\n", "variance decomposition is therefore no longer the same. The tradeoff is better<br>\n", "for bagging: averaging several decision trees fit on bootstrap copies of the<br>\n", "dataset slightly increases the bias term but allows for a larger reduction of<br>\n", "the variance, which results in a lower overall mean squared error (compare the<br>\n", "red curves int the lower figures). The script output also confirms this<br>\n", "intuition. The total error of the bagging ensemble is lower than the total<br>\n", "error of a single decision tree, and this difference indeed mainly stems from a<br>\n", "reduced variance.<br>\n", "For further details on bias-variance decomposition, see section 7.3 of [1]_.<br>\n", "References<br>\n", "----------<br>\n", ".. [1] T. Hastie, R. Tibshirani and J. Friedman,<br>\n", "       \"Elements of Statistical Learning\", Springer, 2009.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Gilles Louppe <g.louppe@gmail.com><br>\n", "License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import BaggingRegressor\n", "from sklearn.tree import DecisionTreeRegressor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Settings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_repeat = 50       # Number of iterations for computing expectations\n", "n_train = 50        # Size of the training set\n", "n_test = 1000       # Size of the test set\n", "noise = 0.1         # Standard deviation of the noise\n", "np.random.seed(0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Change this for exploring the bias-variance decomposition of other<br>\n", "estimators. This should work well for estimators with high variance (e.g.,<br>\n", "decision trees or KNN), but poorly for estimators with low variance (e.g.,<br>\n", "linear models)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["estimators = [(\"Tree\", DecisionTreeRegressor()),\n", "              (\"Bagging(Tree)\", BaggingRegressor(DecisionTreeRegressor()))]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_estimators = len(estimators)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Generate data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def f(x):\n", "    x = x.ravel()\n", "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate(n_samples, noise, n_repeat=1):\n", "    X = np.random.rand(n_samples) * 10 - 5\n", "    X = np.sort(X)\n", "    if n_repeat == 1:\n", "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n", "    else:\n", "        y = np.zeros((n_samples, n_repeat))\n", "        for i in range(n_repeat):\n", "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n", "    X = X.reshape((n_samples, 1))\n", "    return X, y"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = []\n", "y_train = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i in range(n_repeat):\n", "    X, y = generate(n_samples=n_train, noise=noise)\n", "    X_train.append(X)\n", "    y_train.append(y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loop over estimators to compare"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for n, (name, estimator) in enumerate(estimators):\n", "    # Compute predictions\n", "    y_predict = np.zeros((n_test, n_repeat))\n", "    for i in range(n_repeat):\n", "        estimator.fit(X_train[i], y_train[i])\n", "        y_predict[:, i] = estimator.predict(X_test)\n\n", "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n", "    y_error = np.zeros(n_test)\n", "    for i in range(n_repeat):\n", "        for j in range(n_repeat):\n", "            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n", "    y_error /= (n_repeat * n_repeat)\n", "    y_noise = np.var(y_test, axis=1)\n", "    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n", "    y_var = np.var(y_predict, axis=1)\n", "    print(\"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n", "          \" + {3:.4f} (var) + {4:.4f} (noise)\".format(name,\n", "                                                      np.mean(y_error),\n", "                                                      np.mean(y_bias),\n", "                                                      np.mean(y_var),\n", "                                                      np.mean(y_noise)))\n\n", "    # Plot figures\n", "    plt.subplot(2, n_estimators, n + 1)\n", "    plt.plot(X_test, f(X_test), \"b\", label=\"$f(x)$\")\n", "    plt.plot(X_train[0], y_train[0], \".b\", label=\"LS ~ $y = f(x)+noise$\")\n", "    for i in range(n_repeat):\n", "        if i == 0:\n", "            plt.plot(X_test, y_predict[:, i], \"r\", label=r\"$\\^y(x)$\")\n", "        else:\n", "            plt.plot(X_test, y_predict[:, i], \"r\", alpha=0.05)\n", "    plt.plot(X_test, np.mean(y_predict, axis=1), \"c\",\n", "             label=r\"$\\mathbb{E}_{LS} \\^y(x)$\")\n", "    plt.xlim([-5, 5])\n", "    plt.title(name)\n", "    if n == n_estimators - 1:\n", "        plt.legend(loc=(1.1, .5))\n", "    plt.subplot(2, n_estimators, n_estimators + n + 1)\n", "    plt.plot(X_test, y_error, \"r\", label=\"$error(x)$\")\n", "    plt.plot(X_test, y_bias, \"b\", label=\"$bias^2(x)$\"),\n", "    plt.plot(X_test, y_var, \"g\", label=\"$variance(x)$\"),\n", "    plt.plot(X_test, y_noise, \"c\", label=\"$noise(x)$\")\n", "    plt.xlim([-5, 5])\n", "    plt.ylim([0, 0.1])\n", "    if n == n_estimators - 1:\n", "        plt.legend(loc=(1.1, .5))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplots_adjust(right=.75)\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}