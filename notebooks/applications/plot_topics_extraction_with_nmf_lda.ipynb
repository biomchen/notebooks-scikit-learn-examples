{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "=======================================================================================<br>\n", "Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation<br>\n", "=======================================================================================<br>\n", "This is an example of applying :class:`sklearn.decomposition.NMF` and<br>\n", ":class:`sklearn.decomposition.LatentDirichletAllocation` on a corpus<br>\n", "of documents and extract additive models of the topic structure of the<br>\n", "corpus.  The output is a list of topics, each represented as a list of<br>\n", "terms (weights are not shown).<br>\n", "Non-negative Matrix Factorization is applied with two different objective<br>\n", "functions: the Frobenius norm, and the generalized Kullback-Leibler divergence.<br>\n", "The latter is equivalent to Probabilistic Latent Semantic Indexing.<br>\n", "The default parameters (n_samples / n_features / n_components) should make<br>\n", "the example runnable in a couple of tens of seconds. You can try to<br>\n", "increase the dimensions of the problem, but be aware that the time<br>\n", "complexity is polynomial in NMF. In LDA, the time complexity is<br>\n", "proportional to (n_samples * iterations).<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Olivier Grisel <olivier.grisel@ensta.org><br>\n", "        Lars Buitinck<br>\n", "        Chyi-Kwei Yau <chyikwei.yau@gmail.com><br>\n", "License: BSD 3 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from time import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.decomposition import NMF, LatentDirichletAllocation\n", "from sklearn.datasets import fetch_20newsgroups"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples = 2000\n", "n_features = 1000\n", "n_components = 10\n", "n_top_words = 20"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def print_top_words(model, feature_names, n_top_words):\n", "    for topic_idx, topic in enumerate(model.components_):\n", "        message = \"Topic #%d: \" % topic_idx\n", "        message += \" \".join([feature_names[i]\n", "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n", "        print(message)\n", "    print()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the 20 newsgroups dataset and vectorize it. We use a few heuristics<br>\n", "to filter out useless terms early on: the posts are stripped of headers,<br>\n", "footers and quoted replies, and common English words, words occurring in<br>\n", "only one document or in at least 95% of the documents are removed."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Loading dataset...\")\n", "t0 = time()\n", "data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n", "                             remove=('headers', 'footers', 'quotes'),\n", "                             return_X_y=True)\n", "data_samples = data[:n_samples]\n", "print(\"done in %0.3fs.\" % (time() - t0))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use tf-idf features for NMF."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Extracting tf-idf features for NMF...\")\n", "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n", "                                   max_features=n_features,\n", "                                   stop_words='english')\n", "t0 = time()\n", "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n", "print(\"done in %0.3fs.\" % (time() - t0))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use tf (raw term count) features for LDA."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Extracting tf features for LDA...\")\n", "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n", "                                max_features=n_features,\n", "                                stop_words='english')\n", "t0 = time()\n", "tf = tf_vectorizer.fit_transform(data_samples)\n", "print(\"done in %0.3fs.\" % (time() - t0))\n", "print()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the NMF model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n", "      \"n_samples=%d and n_features=%d...\"\n", "      % (n_samples, n_features))\n", "t0 = time()\n", "nmf = NMF(n_components=n_components, random_state=1,\n", "          alpha=.1, l1_ratio=.5).fit(tfidf)\n", "print(\"done in %0.3fs.\" % (time() - t0))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\nTopics in NMF model (Frobenius norm):\")\n", "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n", "print_top_words(nmf, tfidf_feature_names, n_top_words)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the NMF model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n", "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n", "      % (n_samples, n_features))\n", "t0 = time()\n", "nmf = NMF(n_components=n_components, random_state=1,\n", "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n", "          l1_ratio=.5).fit(tfidf)\n", "print(\"done in %0.3fs.\" % (time() - t0))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n", "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n", "print_top_words(nmf, tfidf_feature_names, n_top_words)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Fitting LDA models with tf features, \"\n", "      \"n_samples=%d and n_features=%d...\"\n", "      % (n_samples, n_features))\n", "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n", "                                learning_method='online',\n", "                                learning_offset=50.,\n", "                                random_state=0)\n", "t0 = time()\n", "lda.fit(tf)\n", "print(\"done in %0.3fs.\" % (time() - t0))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\nTopics in LDA model:\")\n", "tf_feature_names = tf_vectorizer.get_feature_names()\n", "print_top_words(lda, tf_feature_names, n_top_words)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}