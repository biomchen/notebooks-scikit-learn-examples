{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["r\n<br>\n", "=====================================================================<br>\n", "The Johnson-Lindenstrauss bound for embedding with random projections<br>\n", "=====================================================================<br>\n", "The `Johnson-Lindenstrauss lemma`_ states that any high dimensional<br>\n", "dataset can be randomly projected into a lower dimensional Euclidean<br>\n", "space while controlling the distortion in the pairwise distances.<br>\n", ".. _`Johnson-Lindenstrauss lemma`: https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "from time import time\n", "import numpy as np\n", "import matplotlib\n", "import matplotlib.pyplot as plt\n", "from distutils.version import LooseVersion\n", "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n", "from sklearn.random_projection import SparseRandomProjection\n", "from sklearn.datasets import fetch_20newsgroups_vectorized\n", "from sklearn.datasets import load_digits\n", "from sklearn.metrics.pairwise import euclidean_distances"]}, {"cell_type": "markdown", "metadata": {}, "source": ["`normed` is being deprecated in favor of `density` in histograms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if LooseVersion(matplotlib.__version__) >= '2.1':\n", "    density_param = {'density': True}\n", "else:\n", "    density_param = {'normed': True}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["########################################################<br>\n", "Theoretical bounds<br>\n", "==================<br>\n", "The distortion introduced by a random projection `p` is asserted by<br>\n", "the fact that `p` is defining an eps-embedding with good probability<br>\n", "as defined by:<br>\n", "<br>\n", ".. math::<br>\n", "   (1 - eps) \\|u - v\\|^2 < \\|p(u) - p(v)\\|^2 < (1 + eps) \\|u - v\\|^2<br>\n", "<br>\n", "Where u and v are any rows taken from a dataset of shape [n_samples,<br>\n", "n_features] and p is a projection by a random Gaussian N(0, 1) matrix<br>\n", "with shape [n_components, n_features] (or a sparse Achlioptas matrix).<br>\n", "<br>\n", "The minimum number of components to guarantees the eps-embedding is<br>\n", "given by:<br>\n", "<br>\n", ".. math::<br>\n", "   n\\_components >= 4 log(n\\_samples) / (eps^2 / 2 - eps^3 / 3)<br>\n", "<br>\n", "<br>\n", "The first plot shows that with an increasing number of samples ``n_samples``,<br>\n", "the minimal number of dimensions ``n_components`` increased logarithmically<br>\n", "in order to guarantee an ``eps``-embedding."]}, {"cell_type": "markdown", "metadata": {}, "source": ["range of admissible distortions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eps_range = np.linspace(0.1, 0.99, 5)\n", "colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["range of number of samples (observation) to embed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples_range = np.logspace(1, 9, 9)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "for eps, color in zip(eps_range, colors):\n", "    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)\n", "    plt.loglog(n_samples_range, min_n_components, color=color)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.legend([\"eps = %0.1f\" % eps for eps in eps_range], loc=\"lower right\")\n", "plt.xlabel(\"Number of observations to eps-embed\")\n", "plt.ylabel(\"Minimum number of dimensions\")\n", "plt.title(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["########################################################<br>\n", "The second plot shows that an increase of the admissible<br>\n", "distortion ``eps`` allows to reduce drastically the minimal number of<br>\n", "dimensions ``n_components`` for a given number of samples ``n_samples``"]}, {"cell_type": "markdown", "metadata": {}, "source": ["range of admissible distortions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eps_range = np.linspace(0.01, 0.99, 100)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["range of number of samples (observation) to embed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples_range = np.logspace(2, 6, 5)\n", "colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "for n_samples, color in zip(n_samples_range, colors):\n", "    min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)\n", "    plt.semilogy(eps_range, min_n_components, color=color)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.legend([\"n_samples = %d\" % n for n in n_samples_range], loc=\"upper right\")\n", "plt.xlabel(\"Distortion eps\")\n", "plt.ylabel(\"Minimum number of dimensions\")\n", "plt.title(\"Johnson-Lindenstrauss bounds:\\nn_components vs eps\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["########################################################<br>\n", "Empirical validation<br>\n", "====================<br>\n", "<br>\n", "We validate the above bounds on the 20 newsgroups text document<br>\n", "(TF-IDF word frequencies) dataset or on the digits dataset:<br>\n", "<br>\n", "- for the 20 newsgroups dataset some 500 documents with 100k<br>\n", "  features in total are projected using a sparse random matrix to smaller<br>\n", "  euclidean spaces with various values for the target number of dimensions<br>\n", "  ``n_components``.<br>\n", "<br>\n", "- for the digits dataset, some 8x8 gray level pixels data for 500<br>\n", "  handwritten digits pictures are randomly projected to spaces for various<br>\n", "  larger number of dimensions ``n_components``.<br>\n", "<br>\n", "The default dataset is the 20 newsgroups dataset. To run the example on the<br>\n", "digits dataset, pass the ``--use-digits-dataset`` command line argument to<br>\n", "this script."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if '--use-digits-dataset' in sys.argv:\n", "    data = load_digits().data[:500]\n", "else:\n", "    data = fetch_20newsgroups_vectorized().data[:500]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["########################################################<br>\n", "For each value of ``n_components``, we plot:<br>\n", "<br>\n", "- 2D distribution of sample pairs with pairwise distances in original<br>\n", "  and projected spaces as x and y axis respectively.<br>\n", "<br>\n", "- 1D histogram of the ratio of those distances (projected / original)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples, n_features = data.shape\n", "print(\"Embedding %d samples with dim %d using various random projections\"\n", "      % (n_samples, n_features))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_components_range = np.array([300, 1000, 10000])\n", "dists = euclidean_distances(data, squared=True).ravel()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["select only non-identical samples pairs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nonzero = dists != 0\n", "dists = dists[nonzero]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for n_components in n_components_range:\n", "    t0 = time()\n", "    rp = SparseRandomProjection(n_components=n_components)\n", "    projected_data = rp.fit_transform(data)\n", "    print(\"Projected %d samples from %d to %d in %0.3fs\"\n", "          % (n_samples, n_features, n_components, time() - t0))\n", "    if hasattr(rp, 'components_'):\n", "        n_bytes = rp.components_.data.nbytes\n", "        n_bytes += rp.components_.indices.nbytes\n", "        print(\"Random matrix with size: %0.3fMB\" % (n_bytes / 1e6))\n", "    projected_dists = euclidean_distances(\n", "        projected_data, squared=True).ravel()[nonzero]\n", "    plt.figure()\n", "    min_dist = min(projected_dists.min(), dists.min())\n", "    max_dist = max(projected_dists.max(), dists.max())\n", "    plt.hexbin(dists, projected_dists, gridsize=100, cmap=plt.cm.PuBu,\n", "               extent=[min_dist, max_dist, min_dist, max_dist])\n", "    plt.xlabel(\"Pairwise squared distances in original space\")\n", "    plt.ylabel(\"Pairwise squared distances in projected space\")\n", "    plt.title(\"Pairwise distances distribution for n_components=%d\" %\n", "              n_components)\n", "    cb = plt.colorbar()\n", "    cb.set_label('Sample pairs counts')\n", "    rates = projected_dists / dists\n", "    print(\"Mean distances rate: %0.2f (%0.2f)\"\n", "          % (np.mean(rates), np.std(rates)))\n", "    plt.figure()\n", "    plt.hist(rates, bins=50, range=(0., 2.), edgecolor='k', **density_param)\n", "    plt.xlabel(\"Squared distances rate: projected / original\")\n", "    plt.ylabel(\"Distribution of samples pairs\")\n", "    plt.title(\"Histogram of pairwise distance rates for n_components=%d\" %\n", "              n_components)\n\n", "    # TODO: compute the expected value of eps and add them to the previous plot\n", "    # as vertical lines / region"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["########################################################<br>\n", "We can see that for low values of ``n_components`` the distribution is wide<br>\n", "with many distorted pairs and a skewed distribution (due to the hard<br>\n", "limit of zero ratio on the left as distances are always positives)<br>\n", "while for larger values of n_components the distortion is controlled<br>\n", "and the distances are well preserved by the random projection."]}, {"cell_type": "markdown", "metadata": {}, "source": ["########################################################<br>\n", "Remarks<br>\n", "=======<br>\n", "<br>\n", "According to the JL lemma, projecting 500 samples without too much distortion<br>\n", "will require at least several thousands dimensions, irrespective of the<br>\n", "number of features of the original dataset.<br>\n", "<br>\n", "Hence using random projections on the digits dataset which only has 64<br>\n", "features in the input space does not make sense: it does not allow<br>\n", "for dimensionality reduction in this case.<br>\n", "<br>\n", "On the twenty newsgroups on the other hand the dimensionality can be<br>\n", "decreased from 56436 down to 10000 while reasonably preserving<br>\n", "pairwise distances."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}