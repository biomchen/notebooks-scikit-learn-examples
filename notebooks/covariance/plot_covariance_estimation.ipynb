{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "=======================================================================<br>\n", "Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood<br>\n", "=======================================================================<br>\n", "When working with covariance estimation, the usual approach is to use<br>\n", "a maximum likelihood estimator, such as the<br>\n", ":class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it<br>\n", "converges to the true (population) covariance when given many<br>\n", "observations. However, it can also be beneficial to regularize it, in<br>\n", "order to reduce its variance; this, in turn, introduces some bias. This<br>\n", "example illustrates the simple regularization used in<br>\n", ":ref:`shrunk_covariance` estimators. In particular, it focuses on how to<br>\n", "set the amount of regularization, i.e. how to choose the bias-variance<br>\n", "trade-off.<br>\n", "Here we compare 3 approaches:<br>\n", "* Setting the parameter by cross-validating the likelihood on three folds<br>\n", "  according to a grid of potential shrinkage parameters.<br>\n", "* A close formula proposed by Ledoit and Wolf to compute<br>\n", "  the asymptotically optimal regularization parameter (minimizing a MSE<br>\n", "  criterion), yielding the :class:`sklearn.covariance.LedoitWolf`<br>\n", "  covariance estimate.<br>\n", "* An improvement of the Ledoit-Wolf shrinkage, the<br>\n", "  :class:`sklearn.covariance.OAS`, proposed by Chen et al. Its<br>\n", "  convergence is significantly better under the assumption that the data<br>\n", "  are Gaussian, in particular for small samples.<br>\n", "To quantify estimation error, we plot the likelihood of unseen data for<br>\n", "different values of the shrinkage parameter. We also show the choices by<br>\n", "cross-validation, or with the LedoitWolf and OAS estimates.<br>\n", "Note that the maximum likelihood estimate corresponds to no shrinkage,<br>\n", "and thus performs poorly. The Ledoit-Wolf estimate performs really well,<br>\n", "as it is close to the optimal and is computational not costly. In this<br>\n", "example, the OAS estimate is a bit further away. Interestingly, both<br>\n", "approaches outperform cross-validation, which is significantly most<br>\n", "computationally costly.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from scipy import linalg"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \\\n", "    log_likelihood, empirical_covariance\n", "from sklearn.model_selection import GridSearchCV"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Generate sample data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_features, n_samples = 40, 20\n", "np.random.seed(42)\n", "base_X_train = np.random.normal(size=(n_samples, n_features))\n", "base_X_test = np.random.normal(size=(n_samples, n_features))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Color samples"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["coloring_matrix = np.random.normal(size=(n_features, n_features))\n", "X_train = np.dot(base_X_train, coloring_matrix)\n", "X_test = np.dot(base_X_test, coloring_matrix)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Compute the likelihood on test data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["spanning a range of possible shrinkage coefficient values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shrinkages = np.logspace(-2, 0, 30)\n", "negative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)\n", "                    for s in shrinkages]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["under the ground-truth model, which we would not have access to in real<br>\n", "settings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["real_cov = np.dot(coloring_matrix.T, coloring_matrix)\n", "emp_cov = empirical_covariance(X_train)\n", "loglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Compare different approaches to setting the parameter"]}, {"cell_type": "markdown", "metadata": {}, "source": ["GridSearch for an optimal shrinkage coefficient"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tuned_parameters = [{'shrinkage': shrinkages}]\n", "cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\n", "cv.fit(X_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ledoit-Wolf optimal shrinkage coefficient estimate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lw = LedoitWolf()\n", "loglik_lw = lw.fit(X_train).score(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["OAS coefficient estimate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["oa = OAS()\n", "loglik_oa = oa.fit(X_train).score(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Plot results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = plt.figure()\n", "plt.title(\"Regularized covariance: likelihood and shrinkage coefficient\")\n", "plt.xlabel('Regularization parameter: shrinkage coefficient')\n", "plt.ylabel('Error: negative log-likelihood on test data')\n", "# range shrinkage curve\n", "plt.loglog(shrinkages, negative_logliks, label=\"Negative log-likelihood\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(plt.xlim(), 2 * [loglik_real], '--r',\n", "         label=\"Real covariance likelihood\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["adjust view"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lik_max = np.amax(negative_logliks)\n", "lik_min = np.amin(negative_logliks)\n", "ymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))\n", "ymax = lik_max + 10. * np.log(lik_max - lik_min)\n", "xmin = shrinkages[0]\n", "xmax = shrinkages[-1]\n", "# LW likelihood\n", "plt.vlines(lw.shrinkage_, ymin, -loglik_lw, color='magenta',\n", "           linewidth=3, label='Ledoit-Wolf estimate')\n", "# OAS likelihood\n", "plt.vlines(oa.shrinkage_, ymin, -loglik_oa, color='purple',\n", "           linewidth=3, label='OAS estimate')\n", "# best CV estimator likelihood\n", "plt.vlines(cv.best_estimator_.shrinkage, ymin,\n", "           -cv.best_estimator_.score(X_test), color='cyan',\n", "           linewidth=3, label='Cross-validation best estimate')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.ylim(ymin, ymax)\n", "plt.xlim(xmin, xmax)\n", "plt.legend()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}