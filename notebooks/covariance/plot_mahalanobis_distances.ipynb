{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["r\n<br>\n", "================================================================<br>\n", "Robust covariance estimation and Mahalanobis distances relevance<br>\n", "================================================================<br>\n", "An example to show covariance estimation with the Mahalanobis<br>\n", "distances on Gaussian distributed data.<br>\n", "For Gaussian distributed data, the distance of an observation<br>\n", ":math:`x_i` to the mode of the distribution can be computed using its<br>\n", "Mahalanobis distance: :math:`d_{(\\mu,\\Sigma)}(x_i)^2 = (x_i -<br>\n", "\\mu)'\\Sigma^{-1}(x_i - \\mu)` where :math:`\\mu` and :math:`\\Sigma` are<br>\n", "the location and the covariance of the underlying Gaussian<br>\n", "distribution.<br>\n", "In practice, :math:`\\mu` and :math:`\\Sigma` are replaced by some<br>\n", "estimates.  The usual covariance maximum likelihood estimate is very<br>\n", "sensitive to the presence of outliers in the data set and therefor,<br>\n", "the corresponding Mahalanobis distances are. One would better have to<br>\n", "use a robust estimator of covariance to guarantee that the estimation is<br>\n", "resistant to \"erroneous\" observations in the data set and that the<br>\n", "associated Mahalanobis distances accurately reflect the true<br>\n", "organisation of the observations.<br>\n", "The Minimum Covariance Determinant estimator is a robust,<br>\n", "high-breakdown point (i.e. it can be used to estimate the covariance<br>\n", "matrix of highly contaminated datasets, up to<br>\n", ":math:`\\frac{n_\\text{samples}-n_\\text{features}-1}{2}` outliers)<br>\n", "estimator of covariance. The idea is to find<br>\n", ":math:`\\frac{n_\\text{samples}+n_\\text{features}+1}{2}`<br>\n", "observations whose empirical covariance has the smallest determinant,<br>\n", "yielding a \"pure\" subset of observations from which to compute<br>\n", "standards estimates of location and covariance.<br>\n", "The Minimum Covariance Determinant estimator (MCD) has been introduced<br>\n", "by P.J.Rousseuw in [1].<br>\n", "This example illustrates how the Mahalanobis distances are affected by<br>\n", "outlying data: observations drawn from a contaminating distribution<br>\n", "are not distinguishable from the observations coming from the real,<br>\n", "Gaussian distribution that one may want to work with. Using MCD-based<br>\n", "Mahalanobis distances, the two populations become<br>\n", "distinguishable. Associated applications are outliers detection,<br>\n", "observations ranking, clustering, ...<br>\n", "For visualization purpose, the cubic root of the Mahalanobis distances<br>\n", "are represented in the boxplot, as Wilson and Hilferty suggest [2]<br>\n", "[1] P. J. Rousseeuw. Least median of squares regression. J. Am<br>\n", "    Stat Ass, 79:871, 1984.<br>\n", "[2] Wilson, E. B., & Hilferty, M. M. (1931). The distribution of chi-square.<br>\n", "    Proceedings of the National Academy of Sciences of the United States<br>\n", "    of America, 17, 684-688.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.covariance import EmpiricalCovariance, MinCovDet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples = 125\n", "n_outliers = 25\n", "n_features = 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["generate data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gen_cov = np.eye(n_features)\n", "gen_cov[0, 0] = 2.\n", "X = np.dot(np.random.randn(n_samples, n_features), gen_cov)\n", "# add some outliers\n", "outliers_cov = np.eye(n_features)\n", "outliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.\n", "X[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["fit a Minimum Covariance Determinant (MCD) robust estimator to data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["robust_cov = MinCovDet().fit(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["compare estimators learnt from the full data set with true parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["emp_cov = EmpiricalCovariance().fit(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Display results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = plt.figure()\n", "plt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Show data set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["subfig1 = plt.subplot(3, 1, 1)\n", "inlier_plot = subfig1.scatter(X[:, 0], X[:, 1],\n", "                              color='black', label='inliers')\n", "outlier_plot = subfig1.scatter(X[:, 0][-n_outliers:], X[:, 1][-n_outliers:],\n", "                               color='red', label='outliers')\n", "subfig1.set_xlim(subfig1.get_xlim()[0], 11.)\n", "subfig1.set_title(\"Mahalanobis distances of a contaminated data set:\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Show contours of the distance functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["xx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n", "                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))\n", "zz = np.c_[xx.ravel(), yy.ravel()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mahal_emp_cov = emp_cov.mahalanobis(zz)\n", "mahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\n", "emp_cov_contour = subfig1.contour(xx, yy, np.sqrt(mahal_emp_cov),\n", "                                  cmap=plt.cm.PuBu_r,\n", "                                  linestyles='dashed')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mahal_robust_cov = robust_cov.mahalanobis(zz)\n", "mahal_robust_cov = mahal_robust_cov.reshape(xx.shape)\n", "robust_contour = subfig1.contour(xx, yy, np.sqrt(mahal_robust_cov),\n", "                                 cmap=plt.cm.YlOrBr_r, linestyles='dotted')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["subfig1.legend([emp_cov_contour.collections[1], robust_contour.collections[1],\n", "                inlier_plot, outlier_plot],\n", "               ['MLE dist', 'robust dist', 'inliers', 'outliers'],\n", "               loc=\"upper right\", borderaxespad=0)\n", "plt.xticks(())\n", "plt.yticks(())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot the scores for each point"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["emp_mahal = emp_cov.mahalanobis(X - np.mean(X, 0)) ** (0.33)\n", "subfig2 = plt.subplot(2, 2, 3)\n", "subfig2.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=.25)\n", "subfig2.plot(np.full(n_samples - n_outliers, 1.26),\n", "             emp_mahal[:-n_outliers], '+k', markeredgewidth=1)\n", "subfig2.plot(np.full(n_outliers, 2.26),\n", "             emp_mahal[-n_outliers:], '+k', markeredgewidth=1)\n", "subfig2.axes.set_xticklabels(('inliers', 'outliers'), size=15)\n", "subfig2.set_ylabel(r\"$\\sqrt[3]{\\rm{(Mahal. dist.)}}$\", size=16)\n", "subfig2.set_title(\"1. from non-robust estimates\\n(Maximum Likelihood)\")\n", "plt.yticks(())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["robust_mahal = robust_cov.mahalanobis(X - robust_cov.location_) ** (0.33)\n", "subfig3 = plt.subplot(2, 2, 4)\n", "subfig3.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]],\n", "                widths=.25)\n", "subfig3.plot(np.full(n_samples - n_outliers, 1.26),\n", "             robust_mahal[:-n_outliers], '+k', markeredgewidth=1)\n", "subfig3.plot(np.full(n_outliers, 2.26),\n", "             robust_mahal[-n_outliers:], '+k', markeredgewidth=1)\n", "subfig3.axes.set_xticklabels(('inliers', 'outliers'), size=15)\n", "subfig3.set_ylabel(r\"$\\sqrt[3]{\\rm{(Mahal. dist.)}}$\", size=16)\n", "subfig3.set_title(\"2. from robust estimates\\n(Minimum Covariance Determinant)\")\n", "plt.yticks(())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}