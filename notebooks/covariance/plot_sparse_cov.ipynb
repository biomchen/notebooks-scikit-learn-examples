{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "======================================<br>\n", "Sparse inverse covariance estimation<br>\n", "======================================<br>\n", "Using the GraphicalLasso estimator to learn a covariance and sparse precision<br>\n", "from a small number of samples.<br>\n", "To estimate a probabilistic model (e.g. a Gaussian model), estimating the<br>\n", "precision matrix, that is the inverse covariance matrix, is as important<br>\n", "as estimating the covariance matrix. Indeed a Gaussian model is<br>\n", "parametrized by the precision matrix.<br>\n", "To be in favorable recovery conditions, we sample the data from a model<br>\n", "with a sparse inverse covariance matrix. In addition, we ensure that the<br>\n", "data is not too much correlated (limiting the largest coefficient of the<br>\n", "precision matrix) and that there a no small coefficients in the<br>\n", "precision matrix that cannot be recovered. In addition, with a small<br>\n", "number of observations, it is easier to recover a correlation matrix<br>\n", "rather than a covariance, thus we scale the time series.<br>\n", "Here, the number of samples is slightly larger than the number of<br>\n", "dimensions, thus the empirical covariance is still invertible. However,<br>\n", "as the observations are strongly correlated, the empirical covariance<br>\n", "matrix is ill-conditioned and as a result its inverse --the empirical<br>\n", "precision matrix-- is very far from the ground truth.<br>\n", "If we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number<br>\n", "of samples is small, we need to shrink a lot. As a result, the<br>\n", "Ledoit-Wolf precision is fairly close to the ground truth precision, that<br>\n", "is not far from being diagonal, but the off-diagonal structure is lost.<br>\n", "The l1-penalized estimator can recover part of this off-diagonal<br>\n", "structure. It learns a sparse precision. It is not able to<br>\n", "recover the exact sparsity pattern: it detects too many non-zero<br>\n", "coefficients. However, the highest non-zero coefficients of the l1<br>\n", "estimated correspond to the non-zero coefficients in the ground truth.<br>\n", "Finally, the coefficients of the l1 precision estimate are biased toward<br>\n", "zero: because of the penalty, they are all smaller than the corresponding<br>\n", "ground truth value, as can be seen on the figure.<br>\n", "Note that, the color range of the precision matrices is tweaked to<br>\n", "improve readability of the figure. The full range of values of the<br>\n", "empirical precision is not displayed.<br>\n", "The alpha parameter of the GraphicalLasso setting the sparsity of the model is<br>\n", "set by internal cross-validation in the GraphicalLassoCV. As can be<br>\n", "seen on figure 2, the grid to compute the cross-validation score is<br>\n", "iteratively refined in the neighborhood of the maximum.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)\n", "# author: Gael Varoquaux <gael.varoquaux@inria.fr>\n", "# License: BSD 3 clause\n", "# Copyright: INRIA"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from scipy import linalg\n", "from sklearn.datasets import make_sparse_spd_matrix\n", "from sklearn.covariance import GraphicalLassoCV, ledoit_wolf\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Generate the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_samples = 60\n", "n_features = 20"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["prng = np.random.RandomState(1)\n", "prec = make_sparse_spd_matrix(n_features, alpha=.98,\n", "                              smallest_coef=.4,\n", "                              largest_coef=.7,\n", "                              random_state=prng)\n", "cov = linalg.inv(prec)\n", "d = np.sqrt(np.diag(cov))\n", "cov /= d\n", "cov /= d[:, np.newaxis]\n", "prec *= d\n", "prec *= d[:, np.newaxis]\n", "X = prng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n", "X -= X.mean(axis=0)\n", "X /= X.std(axis=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Estimate the covariance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["emp_cov = np.dot(X.T, X) / n_samples"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = GraphicalLassoCV()\n", "model.fit(X)\n", "cov_ = model.covariance_\n", "prec_ = model.precision_"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lw_cov_, _ = ledoit_wolf(X)\n", "lw_prec_ = linalg.inv(lw_cov_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Plot the results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 6))\n", "plt.subplots_adjust(left=0.02, right=0.98)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["plot the covariances"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["covs = [('Empirical', emp_cov), ('Ledoit-Wolf', lw_cov_),\n", "        ('GraphicalLassoCV', cov_), ('True', cov)]\n", "vmax = cov_.max()\n", "for i, (name, this_cov) in enumerate(covs):\n", "    plt.subplot(2, 4, i + 1)\n", "    plt.imshow(this_cov, interpolation='nearest', vmin=-vmax, vmax=vmax,\n", "               cmap=plt.cm.RdBu_r)\n", "    plt.xticks(())\n", "    plt.yticks(())\n", "    plt.title('%s covariance' % name)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["plot the precisions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["precs = [('Empirical', linalg.inv(emp_cov)), ('Ledoit-Wolf', lw_prec_),\n", "         ('GraphicalLasso', prec_), ('True', prec)]\n", "vmax = .9 * prec_.max()\n", "for i, (name, this_prec) in enumerate(precs):\n", "    ax = plt.subplot(2, 4, i + 5)\n", "    plt.imshow(np.ma.masked_equal(this_prec, 0),\n", "               interpolation='nearest', vmin=-vmax, vmax=vmax,\n", "               cmap=plt.cm.RdBu_r)\n", "    plt.xticks(())\n", "    plt.yticks(())\n", "    plt.title('%s precision' % name)\n", "    if hasattr(ax, 'set_facecolor'):\n", "        ax.set_facecolor('.7')\n", "    else:\n", "        ax.set_axis_bgcolor('.7')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["plot the model selection metric"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(4, 3))\n", "plt.axes([.2, .15, .75, .7])\n", "plt.plot(model.cv_alphas_, np.mean(model.grid_scores_, axis=1), 'o-')\n", "plt.axvline(model.alpha_, color='.5')\n", "plt.title('Model selection')\n", "plt.ylabel('Cross-validation score')\n", "plt.xlabel('alpha')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}