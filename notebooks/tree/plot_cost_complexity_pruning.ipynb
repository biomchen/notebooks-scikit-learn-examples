{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "========================================================<br>\n", "Post pruning decision trees with cost complexity pruning<br>\n", "========================================================<br>\n", ".. currentmodule:: sklearn.tree<br>\n", "The :class:`DecisionTreeClassifier` provides parameters such as<br>\n", "``min_samples_leaf`` and ``max_depth`` to prevent a tree from overfiting. Cost<br>\n", "complexity pruning provides another option to control the size of a tree. In<br>\n", ":class:`DecisionTreeClassifier`, this pruning technique is parameterized by the<br>\n", "cost complexity parameter, ``ccp_alpha``. Greater values of ``ccp_alpha``<br>\n", "increase the number of nodes pruned. Here we only show the effect of<br>\n", "``ccp_alpha`` on regularizing the trees and how to choose a ``ccp_alpha``<br>\n", "based on validation scores.<br>\n", "See also :ref:`minimal_cost_complexity_pruning` for details on pruning.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.datasets import load_breast_cancer\n", "from sklearn.tree import DecisionTreeClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Total impurity of leaves vs effective alphas of pruned tree<br>\n", "---------------------------------------------------------------<br>\n", "Minimal cost complexity pruning recursively finds the node with the \"weakest<br>\n", "link\". The weakest link is characterized by an effective alpha, where the<br>\n", "nodes with the smallest effective alpha are pruned first. To get an idea of<br>\n", "what values of ``ccp_alpha`` could be appropriate, scikit-learn provides<br>\n", ":func:`DecisionTreeClassifier.cost_complexity_pruning_path` that returns the<br>\n", "effective alphas and the corresponding total leaf impurities at each step of<br>\n", "the pruning process. As alpha increases, more of the tree is pruned, which<br>\n", "increases the total impurity of its leaves."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = load_breast_cancer(return_X_y=True)\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = DecisionTreeClassifier(random_state=0)\n", "path = clf.cost_complexity_pruning_path(X_train, y_train)\n", "ccp_alphas, impurities = path.ccp_alphas, path.impurities"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "In the following plot, the maximum effective alpha value is removed, because<br>\n", "it is the trivial tree with only one node."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n", "ax.set_xlabel(\"effective alpha\")\n", "ax.set_ylabel(\"total impurity of leaves\")\n", "ax.set_title(\"Total Impurity vs effective alpha for training set\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Next, we train a decision tree using the effective alphas. The last value<br>\n", "in ``ccp_alphas`` is the alpha value that prunes the whole tree,<br>\n", "leaving the tree, ``clfs[-1]``, with one node."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clfs = []\n", "for ccp_alpha in ccp_alphas:\n", "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n", "    clf.fit(X_train, y_train)\n", "    clfs.append(clf)\n", "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n", "      clfs[-1].tree_.node_count, ccp_alphas[-1]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "For the remainder of this example, we remove the last element in<br>\n", "``clfs`` and ``ccp_alphas``, because it is the trivial tree with only one<br>\n", "node. Here we show that the number of nodes and tree depth decreases as alpha<br>\n", "increases."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clfs = clfs[:-1]\n", "ccp_alphas = ccp_alphas[:-1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["node_counts = [clf.tree_.node_count for clf in clfs]\n", "depth = [clf.tree_.max_depth for clf in clfs]\n", "fig, ax = plt.subplots(2, 1)\n", "ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\n", "ax[0].set_xlabel(\"alpha\")\n", "ax[0].set_ylabel(\"number of nodes\")\n", "ax[0].set_title(\"Number of nodes vs alpha\")\n", "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\n", "ax[1].set_xlabel(\"alpha\")\n", "ax[1].set_ylabel(\"depth of tree\")\n", "ax[1].set_title(\"Depth vs alpha\")\n", "fig.tight_layout()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Accuracy vs alpha for training and testing sets<br>\n", "----------------------------------------------------<br>\n", "When ``ccp_alpha`` is set to zero and keeping the other default parameters<br>\n", "of :class:`DecisionTreeClassifier`, the tree overfits, leading to<br>\n", "a 100% training accuracy and 88% testing accuracy. As alpha increases, more<br>\n", "of the tree is pruned, thus creating a decision tree that generalizes better.<br>\n", "In this example, setting ``ccp_alpha=0.015`` maximizes the testing accuracy."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_scores = [clf.score(X_train, y_train) for clf in clfs]\n", "test_scores = [clf.score(X_test, y_test) for clf in clfs]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.set_xlabel(\"alpha\")\n", "ax.set_ylabel(\"accuracy\")\n", "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n", "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n", "        drawstyle=\"steps-post\")\n", "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n", "        drawstyle=\"steps-post\")\n", "ax.legend()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}