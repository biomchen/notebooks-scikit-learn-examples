{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "=======================================<br>\n", "Receiver Operating Characteristic (ROC)<br>\n", "=======================================<br>\n", "Example of Receiver Operating Characteristic (ROC) metric to evaluate<br>\n", "classifier output quality.<br>\n", "ROC curves typically feature true positive rate on the Y axis, and false<br>\n", "positive rate on the X axis. This means that the top left corner of the plot is<br>\n", "the \"ideal\" point - a false positive rate of zero, and a true positive rate of<br>\n", "one. This is not very realistic, but it does mean that a larger area under the<br>\n", "curve (AUC) is usually better.<br>\n", "The \"steepness\" of ROC curves is also important, since it is ideal to maximize<br>\n", "the true positive rate while minimizing the false positive rate.<br>\n", "ROC curves are typically used in binary classification to study the output of<br>\n", "a classifier. In order to extend ROC curve and ROC area to multi-label<br>\n", "classification, it is necessary to binarize the output. One ROC<br>\n", "curve can be drawn per label, but one can also draw a ROC curve by considering<br>\n", "each element of the label indicator matrix as a binary prediction<br>\n", "(micro-averaging).<br>\n", "Another evaluation measure for multi-label classification is<br>\n", "macro-averaging, which gives equal weight to the classification of each<br>\n", "label.<br>\n", ".. note::<br>\n", "    See also :func:`sklearn.metrics.roc_auc_score`,<br>\n", "             :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from itertools import cycle"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import svm, datasets\n", "from sklearn.metrics import roc_curve, auc\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import label_binarize\n", "from sklearn.multiclass import OneVsRestClassifier\n", "from scipy import interp\n", "from sklearn.metrics import roc_auc_score"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Import some data to play with"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["iris = datasets.load_iris()\n", "X = iris.data\n", "y = iris.target"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Binarize the output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = label_binarize(y, classes=[0, 1, 2])\n", "n_classes = y.shape[1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Add noisy features to make the problem harder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["random_state = np.random.RandomState(0)\n", "n_samples, n_features = X.shape\n", "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["shuffle and split training and test sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n", "                                                    random_state=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Learn to predict each class against the other"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n", "                                 random_state=random_state))\n", "y_score = classifier.fit(X_train, y_train).decision_function(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute ROC curve and ROC area for each class"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fpr = dict()\n", "tpr = dict()\n", "roc_auc = dict()\n", "for i in range(n_classes):\n", "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n", "    roc_auc[i] = auc(fpr[i], tpr[i])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute micro-average ROC curve and ROC area"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n", "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Plot of a ROC curve for a specific class"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "lw = 2\n", "plt.plot(fpr[2], tpr[2], color='darkorange',\n", "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n", "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n", "plt.xlim([0.0, 1.0])\n", "plt.ylim([0.0, 1.05])\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('Receiver operating characteristic example')\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Plot ROC curves for the multilabel problem<br>\n", "..........................................<br>\n", "Compute macro-average ROC curve and ROC area"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First aggregate all false positive rates"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then interpolate all ROC curves at this points"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_tpr = np.zeros_like(all_fpr)\n", "for i in range(n_classes):\n", "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally average it and compute AUC"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_tpr /= n_classes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fpr[\"macro\"] = all_fpr\n", "tpr[\"macro\"] = mean_tpr\n", "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot all ROC curves"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n", "         label='micro-average ROC curve (area = {0:0.2f})'\n", "               ''.format(roc_auc[\"micro\"]),\n", "         color='deeppink', linestyle=':', linewidth=4)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n", "         label='macro-average ROC curve (area = {0:0.2f})'\n", "               ''.format(roc_auc[\"macro\"]),\n", "         color='navy', linestyle=':', linewidth=4)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n", "for i, color in zip(range(n_classes), colors):\n", "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n", "             label='ROC curve of class {0} (area = {1:0.2f})'\n", "             ''.format(i, roc_auc[i]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n", "plt.xlim([0.0, 1.0])\n", "plt.ylim([0.0, 1.05])\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('Some extension of Receiver operating characteristic to multi-class')\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################<br>\n", "Area under ROC for the multiclass problem<br>\n", ".........................................<br>\n", "The :func:`sklearn.metrics.roc_auc_score` function can be used for<br>\n", "multi-class classification. The multi-class One-vs-One scheme compares every<br>\n", "unique pairwise combination of classes. In this section, we calculate the AUC<br>\n", "using the OvR and OvO schemes. We report a macro average, and a<br>\n", "prevalence-weighted average."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_prob = classifier.predict_proba(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["macro_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\",\n", "                                  average=\"macro\")\n", "weighted_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\",\n", "                                     average=\"weighted\")\n", "macro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n", "                                  average=\"macro\")\n", "weighted_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n", "                                     average=\"weighted\")\n", "print(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n", "      \"(weighted by prevalence)\"\n", "      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\n", "print(\"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n", "      \"(weighted by prevalence)\"\n", "      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}