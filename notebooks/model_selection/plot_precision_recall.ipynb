{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "================<br>\n", "Precision-Recall<br>\n", "================<br>\n", "Example of Precision-Recall metric to evaluate classifier output quality.<br>\n", "Precision-Recall is a useful measure of success of prediction when the<br>\n", "classes are very imbalanced. In information retrieval, precision is a<br>\n", "measure of result relevancy, while recall is a measure of how many truly<br>\n", "relevant results are returned.<br>\n", "The precision-recall curve shows the tradeoff between precision and<br>\n", "recall for different threshold. A high area under the curve represents<br>\n", "both high recall and high precision, where high precision relates to a<br>\n", "low false positive rate, and high recall relates to a low false negative<br>\n", "rate. High scores for both show that the classifier is returning accurate<br>\n", "results (high precision), as well as returning a majority of all positive<br>\n", "results (high recall).<br>\n", "A system with high recall but low precision returns many results, but most of<br>\n", "its predicted labels are incorrect when compared to the training labels. A<br>\n", "system with high precision but low recall is just the opposite, returning very<br>\n", "few results, but most of its predicted labels are correct when compared to the<br>\n", "training labels. An ideal system with high precision and high recall will<br>\n", "return many results, with all results labeled correctly.<br>\n", "Precision (:math:`P`) is defined as the number of true positives (:math:`T_p`)<br>\n", "over the number of true positives plus the number of false positives<br>\n", "(:math:`F_p`).<br>\n", ":math:`P = \\\\frac{T_p}{T_p+F_p}`<br>\n", "Recall (:math:`R`) is defined as the number of true positives (:math:`T_p`)<br>\n", "over the number of true positives plus the number of false negatives<br>\n", "(:math:`F_n`).<br>\n", ":math:`R = \\\\frac{T_p}{T_p + F_n}`<br>\n", "These quantities are also related to the (:math:`F_1`) score, which is defined<br>\n", "as the harmonic mean of precision and recall.<br>\n", ":math:`F1 = 2\\\\frac{P \\\\times R}{P+R}`<br>\n", "Note that the precision may not decrease with recall. The<br>\n", "definition of precision (:math:`\\\\frac{T_p}{T_p + F_p}`) shows that lowering<br>\n", "the threshold of a classifier may increase the denominator, by increasing the<br>\n", "number of results returned. If the threshold was previously set too high, the<br>\n", "new results may all be true positives, which will increase precision. If the<br>\n", "previous threshold was about right or too low, further lowering the threshold<br>\n", "will introduce false positives, decreasing precision.<br>\n", "Recall is defined as :math:`\\\\frac{T_p}{T_p+F_n}`, where :math:`T_p+F_n` does<br>\n", "not depend on the classifier threshold. This means that lowering the classifier<br>\n", "threshold may increase recall, by increasing the number of true positive<br>\n", "results. It is also possible that lowering the threshold may leave recall<br>\n", "unchanged, while the precision fluctuates.<br>\n", "The relationship between recall and precision can be observed in the<br>\n", "stairstep area of the plot - at the edges of these steps a small change<br>\n", "in the threshold considerably reduces precision, with only a minor gain in<br>\n", "recall.<br>\n", "**Average precision** (AP) summarizes such a plot as the weighted mean of<br>\n", "precisions achieved at each threshold, with the increase in recall from the<br>\n", "previous threshold used as the weight:<br>\n", ":math:`\\\\text{AP} = \\\\sum_n (R_n - R_{n-1}) P_n`<br>\n", "where :math:`P_n` and :math:`R_n` are the precision and recall at the<br>\n", "nth threshold. A pair :math:`(R_k, P_k)` is referred to as an<br>\n", "*operating point*.<br>\n", "AP and the trapezoidal area under the operating points<br>\n", "(:func:`sklearn.metrics.auc`) are common ways to summarize a precision-recall<br>\n", "curve that lead to different results. Read more in the<br>\n", ":ref:`User Guide <precision_recall_f_measure_metrics>`.<br>\n", "Precision-recall curves are typically used in binary classification to study<br>\n", "the output of a classifier. In order to extend the precision-recall curve and<br>\n", "average precision to multi-class or multi-label classification, it is necessary<br>\n", "to binarize the output. One curve can be drawn per label, but one can also draw<br>\n", "a precision-recall curve by considering each element of the label indicator<br>\n", "matrix as a binary prediction (micro-averaging).<br>\n", ".. note::<br>\n", "    See also :func:`sklearn.metrics.average_precision_score`,<br>\n", "             :func:`sklearn.metrics.recall_score`,<br>\n", "             :func:`sklearn.metrics.precision_score`,<br>\n", "             :func:`sklearn.metrics.f1_score`<br>\n", "<br>\n", "#############################################################################<br>\n", "In binary classification settings<br>\n", "--------------------------------------------------------<br>\n", "<br>\n", "Create simple data<br>\n", "..................<br>\n", "<br>\n", "Try to differentiate the two first classes of the iris data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import svm, datasets\n", "from sklearn.model_selection import train_test_split\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["iris = datasets.load_iris()\n", "X = iris.data\n", "y = iris.target"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Add noisy features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["random_state = np.random.RandomState(0)\n", "n_samples, n_features = X.shape\n", "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Limit to the two first classes, and split into training and test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n", "                                                    test_size=.5,\n", "                                                    random_state=random_state)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a simple classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["classifier = svm.LinearSVC(random_state=random_state)\n", "classifier.fit(X_train, y_train)\n", "y_score = classifier.decision_function(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Compute the average precision score<br>\n", "..................................."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import average_precision_score\n", "average_precision = average_precision_score(y_test, y_score)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Average precision-recall score: {0:0.2f}'.format(\n", "      average_precision))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Plot the Precision-Recall curve<br>\n", "................................"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import precision_recall_curve\n", "from sklearn.metrics import plot_precision_recall_curve\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["disp = plot_precision_recall_curve(classifier, X_test, y_test)\n", "disp.ax_.set_title('2-class Precision-Recall curve: '\n", "                   'AP={0:0.2f}'.format(average_precision))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "In multi-label settings<br>\n", "------------------------<br>\n", "<br>\n", "Create multi-label data, fit, and predict<br>\n", "...........................................<br>\n", "<br>\n", "We create a multi-label dataset, to illustrate the precision-recall in<br>\n", "multi-label settings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import label_binarize"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use label_binarize to be multi-label like settings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Y = label_binarize(y, classes=[0, 1, 2])\n", "n_classes = Y.shape[1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split into training and test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,\n", "                                                    random_state=random_state)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We use OneVsRestClassifier for multi-label prediction"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.multiclass import OneVsRestClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["classifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))\n", "classifier.fit(X_train, Y_train)\n", "y_score = classifier.decision_function(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "The average precision score in multi-label settings<br>\n", "...................................................."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import precision_recall_curve\n", "from sklearn.metrics import average_precision_score"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For each class"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["precision = dict()\n", "recall = dict()\n", "average_precision = dict()\n", "for i in range(n_classes):\n", "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n", "                                                        y_score[:, i])\n", "    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A \"micro-average\": quantifying score on all classes jointly"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n", "    y_score.ravel())\n", "average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n", "                                                     average=\"micro\")\n", "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n", "      .format(average_precision[\"micro\"]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Plot the micro-averaged Precision-Recall curve<br>\n", "...............................................<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "plt.step(recall['micro'], precision['micro'], where='post')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.xlabel('Recall')\n", "plt.ylabel('Precision')\n", "plt.ylim([0.0, 1.05])\n", "plt.xlim([0.0, 1.0])\n", "plt.title(\n", "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n", "    .format(average_precision[\"micro\"]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Plot Precision-Recall curve for each class and iso-f1 curves<br>\n", ".............................................................<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from itertools import cycle\n", "# setup plot details\n", "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(7, 8))\n", "f_scores = np.linspace(0.2, 0.8, num=4)\n", "lines = []\n", "labels = []\n", "for f_score in f_scores:\n", "    x = np.linspace(0.01, 1)\n", "    y = f_score * x / (2 * x - f_score)\n", "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n", "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lines.append(l)\n", "labels.append('iso-f1 curves')\n", "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n", "lines.append(l)\n", "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n", "              ''.format(average_precision[\"micro\"]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i, color in zip(range(n_classes), colors):\n", "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n", "    lines.append(l)\n", "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n", "                  ''.format(i, average_precision[i]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = plt.gcf()\n", "fig.subplots_adjust(bottom=0.25)\n", "plt.xlim([0.0, 1.0])\n", "plt.ylim([0.0, 1.05])\n", "plt.xlabel('Recall')\n", "plt.ylabel('Precision')\n", "plt.title('Extension of Precision-Recall curve to multi-class')\n", "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}