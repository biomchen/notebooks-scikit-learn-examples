{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "============================================================================<br>\n", "Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV<br>\n", "============================================================================<br>\n", "Multiple metric parameter search can be done by setting the ``scoring``<br>\n", "parameter to a list of metric scorer names or a dict mapping the scorer names<br>\n", "to the scorer callables.<br>\n", "The scores of all the scorers are available in the ``cv_results_`` dict at keys<br>\n", "ending in ``'_<scorer_name>'`` (``'mean_test_precision'``,<br>\n", "``'rank_test_precision'``, etc...)<br>\n", "The ``best_estimator_``, ``best_index_``, ``best_score_`` and ``best_params_``<br>\n", "correspond to the scorer (key) that is set to the ``refit`` attribute.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Raghav RV <rvraghav93@gmail.com><br>\n", "License: BSD"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from matplotlib import pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_hastie_10_2\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import make_scorer\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.tree import DecisionTreeClassifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Running ``GridSearchCV`` using multiple evaluation metrics<br>\n", "----------------------------------------------------------<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = make_hastie_10_2(n_samples=8000, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The scorers can be either be one of the predefined metric strings or a scorer<br>\n", "callable, like the one returned by make_scorer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setting refit='AUC', refits an estimator on the whole dataset with the<br>\n", "parameter setting that has the best cross-validated AUC score.<br>\n", "That estimator is made available at ``gs.best_estimator_`` along with<br>\n", "parameters like ``gs.best_score_``, ``gs.best_params_`` and<br>\n", "``gs.best_index_``"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n", "                  param_grid={'min_samples_split': range(2, 403, 10)},\n", "                  scoring=scoring, refit='AUC', return_train_score=True)\n", "gs.fit(X, y)\n", "results = gs.cv_results_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#############################################################################<br>\n", "Plotting the result<br>\n", "-------------------"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(13, 13))\n", "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n", "          fontsize=16)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.xlabel(\"min_samples_split\")\n", "plt.ylabel(\"Score\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ax = plt.gca()\n", "ax.set_xlim(0, 402)\n", "ax.set_ylim(0.73, 1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get the regular numpy array from the MaskedArray"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_axis = np.array(results['param_min_samples_split'].data, dtype=float)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for scorer, color in zip(sorted(scoring), ['g', 'k']):\n", "    for sample, style in (('train', '--'), ('test', '-')):\n", "        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n", "        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n", "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n", "                        sample_score_mean + sample_score_std,\n", "                        alpha=0.1 if sample == 'test' else 0, color=color)\n", "        ax.plot(X_axis, sample_score_mean, style, color=color,\n", "                alpha=1 if sample == 'test' else 0.7,\n", "                label=\"%s (%s)\" % (scorer, sample))\n", "    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n", "    best_score = results['mean_test_%s' % scorer][best_index]\n\n", "    # Plot a dotted vertical line at the best score for that scorer marked by x\n", "    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n", "            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n", "    # Annotate the best score for that scorer\n", "    ax.annotate(\"%0.2f\" % best_score,\n", "                (X_axis[best_index], best_score + 0.005))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.legend(loc=\"best\")\n", "plt.grid(False)\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}