{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "=========================================================================<br>\n", "Comparing randomized search and grid search for hyperparameter estimation<br>\n", "=========================================================================<br>\n", "Compare randomized search and grid search for optimizing hyperparameters of a<br>\n", "random forest.<br>\n", "All parameters that influence the learning are searched simultaneously<br>\n", "(except for the number of estimators, which poses a time / quality tradeoff).<br>\n", "The randomized search and the grid search explore exactly the same space of<br>\n", "parameters. The result in parameter settings is quite similar, while the run<br>\n", "time for randomized search is drastically lower.<br>\n", "The performance is may slightly worse for the randomized search, and is likely<br>\n", "due to a noise effect and would not carry over to a held-out test set.<br>\n", "Note that in practice, one would not search over this many different parameters<br>\n", "simultaneously using grid search, but pick only the ones deemed most important.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(__doc__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from time import time\n", "import scipy.stats as stats\n", "from sklearn.utils.fixes import loguniform"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n", "from sklearn.datasets import load_digits\n", "from sklearn.linear_model import SGDClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["get some data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = load_digits(return_X_y=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["build a classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = SGDClassifier(loss='hinge', penalty='elasticnet',\n", "                    fit_intercept=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Utility function to report best scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def report(results, n_top=3):\n", "    for i in range(1, n_top + 1):\n", "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n", "        for candidate in candidates:\n", "            print(\"Model with rank: {0}\".format(i))\n", "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n", "                  .format(results['mean_test_score'][candidate],\n", "                          results['std_test_score'][candidate]))\n", "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n", "            print(\"\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["specify parameters and distributions to sample from"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_dist = {'average': [True, False],\n", "              'l1_ratio': stats.uniform(0, 1),\n", "              'alpha': loguniform(1e-4, 1e0)}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["run randomized search"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_iter_search = 20\n", "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n", "                                   n_iter=n_iter_search)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["start = time()\n", "random_search.fit(X, y)\n", "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n", "      \" parameter settings.\" % ((time() - start), n_iter_search))\n", "report(random_search.cv_results_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["use a full grid over all parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_grid = {'average': [True, False],\n", "              'l1_ratio': np.linspace(0, 1, num=10),\n", "              'alpha': np.power(10, np.arange(-4, 1, dtype=float))}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["run grid search"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["grid_search = GridSearchCV(clf, param_grid=param_grid)\n", "start = time()\n", "grid_search.fit(X, y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n", "      % (time() - start, len(grid_search.cv_results_['params'])))\n", "report(grid_search.cv_results_)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}